{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with Recurrent Neural Network (RNN)\n",
    "\n",
    "Acknowledgement: This notebook uses the project TEXGRNN from Max Woolf https://github.com/minimaxir/textgenrnn . Great work Mak !\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Text generation is a challenging problem that even the largest data science teams are still struggling with, so we'll explore some of the most common and accessible methods to solve the problem, starting at a somewhat basic level. The approach we will attempt in this notebook is:\n",
    "\n",
    "* RNN/LSTM\n",
    "\n",
    "Specifically, we will be using the library textgenrnn. textgenrnn is a Python module on top of Keras/TensorFlow which can easily generate text using a pretrained recurrent neural network.  \n",
    "\n",
    "Please install from the command line.\n",
    "> pip install textgenrnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a new model\n",
    "You can train a new model using any modern RNN architecture you want by:\n",
    "* calling train_new_model if supplying texts, or adding a new_model=True parameter if training from a file. If you do, the model will save a config file and a vocab file in addition to the weights, and those must be also loaded into a textgenrnn instances.\n",
    "\n",
    "The config parameters available are:\n",
    "* word_level: Whether to train the model at the word level or character level (default: False)\n",
    "* rnn_layers: Number of recurrent LSTM layers in the model (default: 2)\n",
    "* rnn_size: Number of cells in each LSTM layer (default: 128)\n",
    "* rnn_bidirectional: Whether to use Bidirectional LSTMs, which account for sequences both forwards and backwards. Recommended if the input text follows a specific schema. (default: False)\n",
    "* max_length: Maximum number of previous characters/words to use before predicting the next token. This value should be reduced for word-level models (default: 40)\n",
    "* max_words: Maximum number of words (by frequency) to consider for training (default: 10000)\n",
    "* dim_embeddings: Dimensionality of the character/word embeddings (default: 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/tanpohkeam/anaconda3/envs/tipp_nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/tanpohkeam/anaconda3/envs/tipp_nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/tanpohkeam/anaconda3/envs/tipp_nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/tanpohkeam/anaconda3/envs/tipp_nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:474: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/tanpohkeam/anaconda3/envs/tipp_nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:475: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 texts collected.\n",
      "Training on 49,613 character sequences.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 387 steps\n",
      "Epoch 1/2\n",
      "386/387 [============================>.] - ETA: 58:42 - loss: 1.64 - ETA: 30:45 - loss: 1.62 - ETA: 21:22 - loss: 1.64 - ETA: 16:43 - loss: 1.66 - ETA: 13:55 - loss: 1.64 - ETA: 12:01 - loss: 1.58 - ETA: 10:40 - loss: 1.61 - ETA: 9:39 - loss: 1.5936 - ETA: 8:52 - loss: 1.586 - ETA: 8:13 - loss: 1.554 - ETA: 7:41 - loss: 1.544 - ETA: 7:14 - loss: 1.510 - ETA: 6:51 - loss: 1.487 - ETA: 6:31 - loss: 1.504 - ETA: 6:18 - loss: 1.479 - ETA: 6:03 - loss: 1.465 - ETA: 5:50 - loss: 1.464 - ETA: 5:38 - loss: 1.464 - ETA: 5:27 - loss: 1.466 - ETA: 5:17 - loss: 1.462 - ETA: 5:09 - loss: 1.468 - ETA: 5:03 - loss: 1.464 - ETA: 4:56 - loss: 1.460 - ETA: 4:48 - loss: 1.462 - ETA: 4:42 - loss: 1.447 - ETA: 4:37 - loss: 1.448 - ETA: 4:31 - loss: 1.444 - ETA: 4:26 - loss: 1.439 - ETA: 4:21 - loss: 1.431 - ETA: 4:16 - loss: 1.428 - ETA: 4:11 - loss: 1.420 - ETA: 4:07 - loss: 1.418 - ETA: 4:03 - loss: 1.413 - ETA: 4:00 - loss: 1.410 - ETA: 3:58 - loss: 1.410 - ETA: 3:55 - loss: 1.410 - ETA: 3:52 - loss: 1.407 - ETA: 3:49 - loss: 1.411 - ETA: 3:46 - loss: 1.412 - ETA: 3:44 - loss: 1.409 - ETA: 3:41 - loss: 1.409 - ETA: 3:39 - loss: 1.406 - ETA: 3:37 - loss: 1.409 - ETA: 3:34 - loss: 1.409 - ETA: 3:32 - loss: 1.408 - ETA: 3:30 - loss: 1.407 - ETA: 3:28 - loss: 1.411 - ETA: 3:26 - loss: 1.413 - ETA: 3:24 - loss: 1.409 - ETA: 3:22 - loss: 1.407 - ETA: 3:20 - loss: 1.401 - ETA: 3:18 - loss: 1.400 - ETA: 3:16 - loss: 1.402 - ETA: 3:14 - loss: 1.401 - ETA: 3:13 - loss: 1.400 - ETA: 3:11 - loss: 1.403 - ETA: 3:10 - loss: 1.400 - ETA: 3:08 - loss: 1.400 - ETA: 3:06 - loss: 1.404 - ETA: 3:05 - loss: 1.403 - ETA: 3:03 - loss: 1.402 - ETA: 3:02 - loss: 1.404 - ETA: 3:00 - loss: 1.404 - ETA: 2:59 - loss: 1.404 - ETA: 2:58 - loss: 1.402 - ETA: 2:57 - loss: 1.400 - ETA: 2:55 - loss: 1.399 - ETA: 2:54 - loss: 1.398 - ETA: 2:53 - loss: 1.399 - ETA: 2:52 - loss: 1.396 - ETA: 2:51 - loss: 1.397 - ETA: 2:49 - loss: 1.397 - ETA: 2:48 - loss: 1.398 - ETA: 2:47 - loss: 1.395 - ETA: 2:46 - loss: 1.395 - ETA: 2:45 - loss: 1.393 - ETA: 2:44 - loss: 1.393 - ETA: 2:43 - loss: 1.391 - ETA: 2:42 - loss: 1.391 - ETA: 2:41 - loss: 1.389 - ETA: 2:40 - loss: 1.390 - ETA: 2:39 - loss: 1.389 - ETA: 2:38 - loss: 1.385 - ETA: 2:37 - loss: 1.383 - ETA: 2:36 - loss: 1.381 - ETA: 2:35 - loss: 1.377 - ETA: 2:34 - loss: 1.376 - ETA: 2:33 - loss: 1.373 - ETA: 2:33 - loss: 1.372 - ETA: 2:32 - loss: 1.375 - ETA: 2:31 - loss: 1.374 - ETA: 2:30 - loss: 1.373 - ETA: 2:29 - loss: 1.373 - ETA: 2:28 - loss: 1.373 - ETA: 2:27 - loss: 1.375 - ETA: 2:26 - loss: 1.372 - ETA: 2:26 - loss: 1.368 - ETA: 2:25 - loss: 1.366 - ETA: 2:25 - loss: 1.365 - ETA: 2:24 - loss: 1.365 - ETA: 2:23 - loss: 1.363 - ETA: 2:22 - loss: 1.359 - ETA: 2:21 - loss: 1.358 - ETA: 2:21 - loss: 1.360 - ETA: 2:20 - loss: 1.360 - ETA: 2:19 - loss: 1.359 - ETA: 2:18 - loss: 1.355 - ETA: 2:18 - loss: 1.357 - ETA: 2:17 - loss: 1.356 - ETA: 2:17 - loss: 1.356 - ETA: 2:16 - loss: 1.356 - ETA: 2:15 - loss: 1.353 - ETA: 2:15 - loss: 1.352 - ETA: 2:14 - loss: 1.352 - ETA: 2:13 - loss: 1.351 - ETA: 2:13 - loss: 1.351 - ETA: 2:12 - loss: 1.349 - ETA: 2:12 - loss: 1.348 - ETA: 2:11 - loss: 1.347 - ETA: 2:10 - loss: 1.345 - ETA: 2:10 - loss: 1.342 - ETA: 2:09 - loss: 1.342 - ETA: 2:08 - loss: 1.341 - ETA: 2:07 - loss: 1.341 - ETA: 2:07 - loss: 1.341 - ETA: 2:06 - loss: 1.340 - ETA: 2:06 - loss: 1.339 - ETA: 2:05 - loss: 1.337 - ETA: 2:04 - loss: 1.337 - ETA: 2:04 - loss: 1.338 - ETA: 2:03 - loss: 1.338 - ETA: 2:02 - loss: 1.336 - ETA: 2:02 - loss: 1.334 - ETA: 2:01 - loss: 1.334 - ETA: 2:00 - loss: 1.333 - ETA: 2:00 - loss: 1.334 - ETA: 1:59 - loss: 1.333 - ETA: 1:59 - loss: 1.332 - ETA: 1:58 - loss: 1.330 - ETA: 1:57 - loss: 1.331 - ETA: 1:57 - loss: 1.330 - ETA: 1:56 - loss: 1.331 - ETA: 1:55 - loss: 1.331 - ETA: 1:55 - loss: 1.332 - ETA: 1:54 - loss: 1.330 - ETA: 1:54 - loss: 1.332 - ETA: 1:53 - loss: 1.331 - ETA: 1:53 - loss: 1.330 - ETA: 1:52 - loss: 1.329 - ETA: 1:51 - loss: 1.329 - ETA: 1:51 - loss: 1.329 - ETA: 1:50 - loss: 1.327 - ETA: 1:50 - loss: 1.327 - ETA: 1:49 - loss: 1.326 - ETA: 1:48 - loss: 1.325 - ETA: 1:48 - loss: 1.326 - ETA: 1:47 - loss: 1.327 - ETA: 1:47 - loss: 1.327 - ETA: 1:46 - loss: 1.326 - ETA: 1:46 - loss: 1.324 - ETA: 1:45 - loss: 1.323 - ETA: 1:45 - loss: 1.323 - ETA: 1:44 - loss: 1.323 - ETA: 1:43 - loss: 1.323 - ETA: 1:43 - loss: 1.322 - ETA: 1:43 - loss: 1.321 - ETA: 1:42 - loss: 1.321 - ETA: 1:42 - loss: 1.322 - ETA: 1:41 - loss: 1.321 - ETA: 1:41 - loss: 1.320 - ETA: 1:40 - loss: 1.321 - ETA: 1:40 - loss: 1.322 - ETA: 1:39 - loss: 1.324 - ETA: 1:39 - loss: 1.323 - ETA: 1:38 - loss: 1.323 - ETA: 1:38 - loss: 1.322 - ETA: 1:37 - loss: 1.320 - ETA: 1:37 - loss: 1.321 - ETA: 1:36 - loss: 1.321 - ETA: 1:36 - loss: 1.321 - ETA: 1:35 - loss: 1.321 - ETA: 1:34 - loss: 1.320 - ETA: 1:34 - loss: 1.320 - ETA: 1:33 - loss: 1.320 - ETA: 1:33 - loss: 1.318 - ETA: 1:32 - loss: 1.318 - ETA: 1:32 - loss: 1.318 - ETA: 1:31 - loss: 1.318 - ETA: 1:31 - loss: 1.318 - ETA: 1:30 - loss: 1.319 - ETA: 1:30 - loss: 1.318 - ETA: 1:29 - loss: 1.316 - ETA: 1:29 - loss: 1.316 - ETA: 1:28 - loss: 1.316 - ETA: 1:28 - loss: 1.316 - ETA: 1:27 - loss: 1.315 - ETA: 1:27 - loss: 1.315 - ETA: 1:26 - loss: 1.315 - ETA: 1:26 - loss: 1.314 - ETA: 1:25 - loss: 1.314 - ETA: 1:25 - loss: 1.313 - ETA: 1:24 - loss: 1.313 - ETA: 1:24 - loss: 1.314 - ETA: 1:23 - loss: 1.314 - ETA: 1:23 - loss: 1.314 - ETA: 1:22 - loss: 1.313 - ETA: 1:22 - loss: 1.313 - ETA: 1:21 - loss: 1.312 - ETA: 1:21 - loss: 1.312 - ETA: 1:20 - loss: 1.312 - ETA: 1:20 - loss: 1.311 - ETA: 1:19 - loss: 1.311 - ETA: 1:19 - loss: 1.311 - ETA: 1:18 - loss: 1.311 - ETA: 1:18 - loss: 1.311 - ETA: 1:17 - loss: 1.310 - ETA: 1:17 - loss: 1.311 - ETA: 1:16 - loss: 1.311 - ETA: 1:16 - loss: 1.310 - ETA: 1:15 - loss: 1.309 - ETA: 1:15 - loss: 1.309 - ETA: 1:14 - loss: 1.308 - ETA: 1:14 - loss: 1.308 - ETA: 1:13 - loss: 1.307 - ETA: 1:13 - loss: 1.307 - ETA: 1:12 - loss: 1.306 - ETA: 1:12 - loss: 1.306 - ETA: 1:11 - loss: 1.306 - ETA: 1:11 - loss: 1.306 - ETA: 1:10 - loss: 1.306 - ETA: 1:10 - loss: 1.306 - ETA: 1:09 - loss: 1.306 - ETA: 1:09 - loss: 1.306 - ETA: 1:08 - loss: 1.307 - ETA: 1:08 - loss: 1.306 - ETA: 1:07 - loss: 1.305 - ETA: 1:07 - loss: 1.305 - ETA: 1:06 - loss: 1.304 - ETA: 1:06 - loss: 1.304 - ETA: 1:05 - loss: 1.303 - ETA: 1:05 - loss: 1.303 - ETA: 1:05 - loss: 1.303 - ETA: 1:04 - loss: 1.303 - ETA: 1:04 - loss: 1.302 - ETA: 1:03 - loss: 1.302 - ETA: 1:03 - loss: 1.301 - ETA: 1:02 - loss: 1.301 - ETA: 1:02 - loss: 1.302 - ETA: 1:01 - loss: 1.302 - ETA: 1:01 - loss: 1.303 - ETA: 1:00 - loss: 1.301 - ETA: 1:00 - loss: 1.301 - ETA: 59s - loss: 1.302 - ETA: 59s - loss: 1.30 - ETA: 58s - loss: 1.30 - ETA: 58s - loss: 1.30 - ETA: 58s - loss: 1.30 - ETA: 57s - loss: 1.30 - ETA: 57s - loss: 1.30 - ETA: 56s - loss: 1.30 - ETA: 56s - loss: 1.30 - ETA: 55s - loss: 1.30 - ETA: 55s - loss: 1.30 - ETA: 54s - loss: 1.30 - ETA: 54s - loss: 1.30 - ETA: 53s - loss: 1.30 - ETA: 53s - loss: 1.30 - ETA: 52s - loss: 1.30 - ETA: 52s - loss: 1.29 - ETA: 51s - loss: 1.29 - ETA: 51s - loss: 1.29 - ETA: 51s - loss: 1.29 - ETA: 50s - loss: 1.29 - ETA: 50s - loss: 1.29 - ETA: 49s - loss: 1.29 - ETA: 49s - loss: 1.29 - ETA: 48s - loss: 1.29 - ETA: 48s - loss: 1.29 - ETA: 47s - loss: 1.29 - ETA: 47s - loss: 1.29 - ETA: 46s - loss: 1.29 - ETA: 46s - loss: 1.29 - ETA: 46s - loss: 1.29 - ETA: 45s - loss: 1.29 - ETA: 45s - loss: 1.29 - ETA: 44s - loss: 1.29 - ETA: 44s - loss: 1.29 - ETA: 43s - loss: 1.29 - ETA: 43s - loss: 1.29 - ETA: 42s - loss: 1.29 - ETA: 42s - loss: 1.29 - ETA: 42s - loss: 1.29 - ETA: 41s - loss: 1.29 - ETA: 41s - loss: 1.29 - ETA: 40s - loss: 1.29 - ETA: 40s - loss: 1.29 - ETA: 39s - loss: 1.29 - ETA: 39s - loss: 1.29 - ETA: 38s - loss: 1.29 - ETA: 38s - loss: 1.29 - ETA: 37s - loss: 1.29 - ETA: 37s - loss: 1.29 - ETA: 36s - loss: 1.29 - ETA: 36s - loss: 1.29 - ETA: 36s - loss: 1.29 - ETA: 35s - loss: 1.29 - ETA: 35s - loss: 1.29 - ETA: 34s - loss: 1.29 - ETA: 34s - loss: 1.29 - ETA: 33s - loss: 1.29 - ETA: 33s - loss: 1.29 - ETA: 32s - loss: 1.29 - ETA: 32s - loss: 1.29 - ETA: 32s - loss: 1.29 - ETA: 31s - loss: 1.29 - ETA: 31s - loss: 1.29 - ETA: 30s - loss: 1.29 - ETA: 30s - loss: 1.29 - ETA: 29s - loss: 1.29 - ETA: 29s - loss: 1.29 - ETA: 28s - loss: 1.29 - ETA: 28s - loss: 1.29 - ETA: 28s - loss: 1.29 - ETA: 27s - loss: 1.29 - ETA: 27s - loss: 1.29 - ETA: 26s - loss: 1.29 - ETA: 26s - loss: 1.29 - ETA: 25s - loss: 1.29 - ETA: 25s - loss: 1.28 - ETA: 25s - loss: 1.28 - ETA: 24s - loss: 1.28 - ETA: 24s - loss: 1.28 - ETA: 23s - loss: 1.28 - ETA: 23s - loss: 1.28 - ETA: 22s - loss: 1.28 - ETA: 22s - loss: 1.28 - ETA: 21s - loss: 1.28 - ETA: 21s - loss: 1.28 - ETA: 21s - loss: 1.28 - ETA: 20s - loss: 1.28 - ETA: 20s - loss: 1.28 - ETA: 19s - loss: 1.28 - ETA: 19s - loss: 1.28 - ETA: 18s - loss: 1.28 - ETA: 18s - loss: 1.28 - ETA: 17s - loss: 1.28 - ETA: 17s - loss: 1.28 - ETA: 17s - loss: 1.28 - ETA: 16s - loss: 1.28 - ETA: 16s - loss: 1.28 - ETA: 15s - loss: 1.28 - ETA: 15s - loss: 1.28 - ETA: 14s - loss: 1.28 - ETA: 14s - loss: 1.28 - ETA: 13s - loss: 1.28 - ETA: 13s - loss: 1.28 - ETA: 13s - loss: 1.28 - ETA: 12s - loss: 1.28 - ETA: 12s - loss: 1.28 - ETA: 11s - loss: 1.28 - ETA: 11s - loss: 1.28 - ETA: 10s - loss: 1.28 - ETA: 10s - loss: 1.28 - ETA: 10s - loss: 1.28 - ETA: 9s - loss: 1.2842 - ETA: 9s - loss: 1.284 - ETA: 8s - loss: 1.284 - ETA: 8s - loss: 1.283 - ETA: 7s - loss: 1.283 - ETA: 7s - loss: 1.283 - ETA: 6s - loss: 1.283 - ETA: 6s - loss: 1.283 - ETA: 6s - loss: 1.283 - ETA: 5s - loss: 1.282 - ETA: 5s - loss: 1.282 - ETA: 4s - loss: 1.283 - ETA: 4s - loss: 1.283 - ETA: 3s - loss: 1.283 - ETA: 3s - loss: 1.283 - ETA: 3s - loss: 1.282 - ETA: 2s - loss: 1.282 - ETA: 2s - loss: 1.282 - ETA: 1s - loss: 1.282 - ETA: 1s - loss: 1.281 - ETA: 0s - loss: 1.281 - ETA: 0s - loss: 1.2817####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "so that we are some of the team worksheet which we are also as it would be a project of the project and some of the project that we had a project and we can see the project and we had the worksheet where we see the worksheet when the worksheet is a project and we had the worksheet and we see the wo\n",
      "\n",
      "what we had to contribute the project and the project that we had the project and a project and we can help the project and we had the project and the project and the schedules and the project when the school was submitting the project that we had the project that we had the project and some of the\n",
      "\n",
      "what we are doing the worksheet and a whatsapp and we can contribute the project and we can help and we see the worksheet and we had ensure the project where we discuss on the project and stakeholder and some of the project and the team worksheet that we had the project and we can have a contribute\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "we died a quest what we can help the worksheet and we can be able to get the plan project\n",
      "\n",
      "what we did not see what we are helping to contribute in the project and stakeholder and how to contribute the worksheet and we can help understand a scope of the submitting ensure have in the first plan and inight in our project in the worksheet, we this we discuss the discussion where we started \n",
      "\n",
      "the project made converting the submitting the questions what we completed the team worksheet and adding the worksheet and the project documents and use on the project is month and the worksheet is not the project and when we the reading to other, what we would not have the plan must and we discuss\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "as _trying a user encry the splittlespect with the jendufling they on and we discuss the disage and today\n",
      "\n",
      "colore in the sungh on the\n",
      "\n",
      "when might plan us sunartahlath who good which equalth are platfor and groupen in But we are the ensual week 13 group. Where All when he messueedly sourching the faction why we can unplann on the spling, programming nerve. Ever rounds and sure we can quit creating wheres to grong some of them\n",
      "\n",
      "387/387 [==============================] - 318s 821ms/step - loss: 1.2815\n",
      "Epoch 2/2\n",
      "386/387 [============================>.] - ETA: 2:44 - loss: 1.079 - ETA: 2:49 - loss: 1.140 - ETA: 2:39 - loss: 1.024 - ETA: 2:55 - loss: 1.059 - ETA: 2:48 - loss: 1.049 - ETA: 2:45 - loss: 1.071 - ETA: 2:45 - loss: 1.077 - ETA: 2:41 - loss: 1.067 - ETA: 2:39 - loss: 1.065 - ETA: 2:37 - loss: 1.058 - ETA: 2:35 - loss: 1.056 - ETA: 2:33 - loss: 1.060 - ETA: 2:33 - loss: 1.060 - ETA: 2:31 - loss: 1.067 - ETA: 2:30 - loss: 1.067 - ETA: 2:30 - loss: 1.073 - ETA: 2:31 - loss: 1.075 - ETA: 2:31 - loss: 1.081 - ETA: 2:30 - loss: 1.090 - ETA: 2:29 - loss: 1.090 - ETA: 2:28 - loss: 1.089 - ETA: 2:28 - loss: 1.093 - ETA: 2:29 - loss: 1.091 - ETA: 2:30 - loss: 1.092 - ETA: 2:29 - loss: 1.091 - ETA: 2:28 - loss: 1.088 - ETA: 2:27 - loss: 1.090 - ETA: 2:26 - loss: 1.087 - ETA: 2:26 - loss: 1.093 - ETA: 2:25 - loss: 1.090 - ETA: 2:24 - loss: 1.092 - ETA: 2:23 - loss: 1.088 - ETA: 2:23 - loss: 1.088 - ETA: 2:22 - loss: 1.092 - ETA: 2:22 - loss: 1.098 - ETA: 2:21 - loss: 1.096 - ETA: 2:20 - loss: 1.099 - ETA: 2:20 - loss: 1.094 - ETA: 2:19 - loss: 1.092 - ETA: 2:19 - loss: 1.091 - ETA: 2:18 - loss: 1.090 - ETA: 2:18 - loss: 1.093 - ETA: 2:17 - loss: 1.097 - ETA: 2:16 - loss: 1.098 - ETA: 2:16 - loss: 1.096 - ETA: 2:16 - loss: 1.095 - ETA: 2:15 - loss: 1.096 - ETA: 2:15 - loss: 1.098 - ETA: 2:14 - loss: 1.095 - ETA: 2:14 - loss: 1.093 - ETA: 2:14 - loss: 1.094 - ETA: 2:13 - loss: 1.095 - ETA: 2:12 - loss: 1.095 - ETA: 2:12 - loss: 1.093 - ETA: 2:11 - loss: 1.093 - ETA: 2:11 - loss: 1.092 - ETA: 2:11 - loss: 1.094 - ETA: 2:10 - loss: 1.092 - ETA: 2:09 - loss: 1.088 - ETA: 2:09 - loss: 1.084 - ETA: 2:09 - loss: 1.085 - ETA: 2:08 - loss: 1.086 - ETA: 2:08 - loss: 1.086 - ETA: 2:07 - loss: 1.088 - ETA: 2:07 - loss: 1.088 - ETA: 2:06 - loss: 1.086 - ETA: 2:06 - loss: 1.086 - ETA: 2:05 - loss: 1.087 - ETA: 2:05 - loss: 1.088 - ETA: 2:04 - loss: 1.090 - ETA: 2:04 - loss: 1.091 - ETA: 2:03 - loss: 1.087 - ETA: 2:03 - loss: 1.088 - ETA: 2:03 - loss: 1.086 - ETA: 2:02 - loss: 1.085 - ETA: 2:02 - loss: 1.085 - ETA: 2:01 - loss: 1.084 - ETA: 2:01 - loss: 1.084 - ETA: 2:01 - loss: 1.085 - ETA: 2:00 - loss: 1.083 - ETA: 2:00 - loss: 1.083 - ETA: 2:00 - loss: 1.083 - ETA: 1:59 - loss: 1.084 - ETA: 1:59 - loss: 1.083 - ETA: 1:59 - loss: 1.081 - ETA: 1:58 - loss: 1.080 - ETA: 1:58 - loss: 1.082 - ETA: 1:57 - loss: 1.081 - ETA: 1:57 - loss: 1.080 - ETA: 1:56 - loss: 1.083 - ETA: 1:56 - loss: 1.081 - ETA: 1:55 - loss: 1.079 - ETA: 1:55 - loss: 1.080 - ETA: 1:55 - loss: 1.081 - ETA: 1:54 - loss: 1.084 - ETA: 1:54 - loss: 1.084 - ETA: 1:53 - loss: 1.081 - ETA: 1:53 - loss: 1.082 - ETA: 1:53 - loss: 1.081 - ETA: 1:52 - loss: 1.081 - ETA: 1:52 - loss: 1.082 - ETA: 1:52 - loss: 1.080 - ETA: 1:51 - loss: 1.079 - ETA: 1:51 - loss: 1.078 - ETA: 1:50 - loss: 1.080 - ETA: 1:50 - loss: 1.081 - ETA: 1:50 - loss: 1.080 - ETA: 1:49 - loss: 1.080 - ETA: 1:49 - loss: 1.081 - ETA: 1:48 - loss: 1.081 - ETA: 1:48 - loss: 1.083 - ETA: 1:48 - loss: 1.086 - ETA: 1:47 - loss: 1.085 - ETA: 1:47 - loss: 1.084 - ETA: 1:46 - loss: 1.084 - ETA: 1:46 - loss: 1.083 - ETA: 1:46 - loss: 1.083 - ETA: 1:45 - loss: 1.083 - ETA: 1:45 - loss: 1.084 - ETA: 1:45 - loss: 1.085 - ETA: 1:44 - loss: 1.086 - ETA: 1:44 - loss: 1.085 - ETA: 1:43 - loss: 1.084 - ETA: 1:43 - loss: 1.085 - ETA: 1:43 - loss: 1.086 - ETA: 1:42 - loss: 1.086 - ETA: 1:42 - loss: 1.087 - ETA: 1:41 - loss: 1.086 - ETA: 1:41 - loss: 1.088 - ETA: 1:41 - loss: 1.089 - ETA: 1:40 - loss: 1.089 - ETA: 1:40 - loss: 1.089 - ETA: 1:39 - loss: 1.089 - ETA: 1:39 - loss: 1.090 - ETA: 1:38 - loss: 1.089 - ETA: 1:38 - loss: 1.090 - ETA: 1:38 - loss: 1.089 - ETA: 1:37 - loss: 1.089 - ETA: 1:37 - loss: 1.089 - ETA: 1:36 - loss: 1.091 - ETA: 1:36 - loss: 1.091 - ETA: 1:36 - loss: 1.090 - ETA: 1:35 - loss: 1.091 - ETA: 1:35 - loss: 1.090 - ETA: 1:34 - loss: 1.089 - ETA: 1:34 - loss: 1.089 - ETA: 1:33 - loss: 1.088 - ETA: 1:33 - loss: 1.088 - ETA: 1:33 - loss: 1.088 - ETA: 1:32 - loss: 1.088 - ETA: 1:32 - loss: 1.088 - ETA: 1:31 - loss: 1.087 - ETA: 1:31 - loss: 1.085 - ETA: 1:31 - loss: 1.086 - ETA: 1:30 - loss: 1.085 - ETA: 1:30 - loss: 1.085 - ETA: 1:29 - loss: 1.084 - ETA: 1:29 - loss: 1.084 - ETA: 1:29 - loss: 1.083 - ETA: 1:28 - loss: 1.082 - ETA: 1:28 - loss: 1.082 - ETA: 1:27 - loss: 1.081 - ETA: 1:27 - loss: 1.080 - ETA: 1:27 - loss: 1.080 - ETA: 1:26 - loss: 1.081 - ETA: 1:26 - loss: 1.079 - ETA: 1:25 - loss: 1.079 - ETA: 1:25 - loss: 1.079 - ETA: 1:25 - loss: 1.080 - ETA: 1:24 - loss: 1.080 - ETA: 1:24 - loss: 1.079 - ETA: 1:24 - loss: 1.078 - ETA: 1:23 - loss: 1.078 - ETA: 1:23 - loss: 1.076 - ETA: 1:22 - loss: 1.076 - ETA: 1:22 - loss: 1.077 - ETA: 1:22 - loss: 1.076 - ETA: 1:21 - loss: 1.076 - ETA: 1:21 - loss: 1.077 - ETA: 1:20 - loss: 1.076 - ETA: 1:20 - loss: 1.077 - ETA: 1:20 - loss: 1.076 - ETA: 1:19 - loss: 1.075 - ETA: 1:19 - loss: 1.074 - ETA: 1:18 - loss: 1.074 - ETA: 1:18 - loss: 1.074 - ETA: 1:18 - loss: 1.075 - ETA: 1:17 - loss: 1.075 - ETA: 1:17 - loss: 1.074 - ETA: 1:16 - loss: 1.075 - ETA: 1:16 - loss: 1.075 - ETA: 1:16 - loss: 1.076 - ETA: 1:15 - loss: 1.076 - ETA: 1:15 - loss: 1.076 - ETA: 1:14 - loss: 1.076 - ETA: 1:14 - loss: 1.076 - ETA: 1:14 - loss: 1.076 - ETA: 1:13 - loss: 1.076 - ETA: 1:13 - loss: 1.076 - ETA: 1:12 - loss: 1.075 - ETA: 1:12 - loss: 1.075 - ETA: 1:12 - loss: 1.075 - ETA: 1:11 - loss: 1.075 - ETA: 1:11 - loss: 1.075 - ETA: 1:10 - loss: 1.075 - ETA: 1:10 - loss: 1.075 - ETA: 1:10 - loss: 1.077 - ETA: 1:09 - loss: 1.076 - ETA: 1:09 - loss: 1.076 - ETA: 1:09 - loss: 1.077 - ETA: 1:08 - loss: 1.077 - ETA: 1:08 - loss: 1.077 - ETA: 1:07 - loss: 1.077 - ETA: 1:07 - loss: 1.077 - ETA: 1:07 - loss: 1.078 - ETA: 1:06 - loss: 1.077 - ETA: 1:06 - loss: 1.079 - ETA: 1:05 - loss: 1.079 - ETA: 1:05 - loss: 1.079 - ETA: 1:05 - loss: 1.079 - ETA: 1:04 - loss: 1.079 - ETA: 1:04 - loss: 1.078 - ETA: 1:03 - loss: 1.078 - ETA: 1:03 - loss: 1.078 - ETA: 1:03 - loss: 1.078 - ETA: 1:02 - loss: 1.078 - ETA: 1:02 - loss: 1.078 - ETA: 1:01 - loss: 1.077 - ETA: 1:01 - loss: 1.077 - ETA: 1:01 - loss: 1.077 - ETA: 1:00 - loss: 1.078 - ETA: 1:00 - loss: 1.078 - ETA: 59s - loss: 1.077 - ETA: 59s - loss: 1.07 - ETA: 59s - loss: 1.07 - ETA: 58s - loss: 1.07 - ETA: 58s - loss: 1.07 - ETA: 57s - loss: 1.07 - ETA: 57s - loss: 1.07 - ETA: 57s - loss: 1.07 - ETA: 56s - loss: 1.07 - ETA: 56s - loss: 1.07 - ETA: 55s - loss: 1.07 - ETA: 55s - loss: 1.07 - ETA: 55s - loss: 1.07 - ETA: 54s - loss: 1.07 - ETA: 54s - loss: 1.07 - ETA: 53s - loss: 1.07 - ETA: 53s - loss: 1.07 - ETA: 53s - loss: 1.07 - ETA: 52s - loss: 1.07 - ETA: 52s - loss: 1.07 - ETA: 51s - loss: 1.07 - ETA: 51s - loss: 1.07 - ETA: 51s - loss: 1.07 - ETA: 50s - loss: 1.07 - ETA: 50s - loss: 1.07 - ETA: 50s - loss: 1.07 - ETA: 49s - loss: 1.07 - ETA: 49s - loss: 1.07 - ETA: 48s - loss: 1.07 - ETA: 48s - loss: 1.07 - ETA: 48s - loss: 1.07 - ETA: 47s - loss: 1.07 - ETA: 47s - loss: 1.07 - ETA: 46s - loss: 1.07 - ETA: 46s - loss: 1.07 - ETA: 46s - loss: 1.07 - ETA: 45s - loss: 1.07 - ETA: 45s - loss: 1.07 - ETA: 45s - loss: 1.07 - ETA: 44s - loss: 1.07 - ETA: 44s - loss: 1.07 - ETA: 43s - loss: 1.07 - ETA: 43s - loss: 1.07 - ETA: 43s - loss: 1.07 - ETA: 42s - loss: 1.07 - ETA: 42s - loss: 1.07 - ETA: 41s - loss: 1.07 - ETA: 41s - loss: 1.07 - ETA: 41s - loss: 1.07 - ETA: 40s - loss: 1.07 - ETA: 40s - loss: 1.07 - ETA: 39s - loss: 1.07 - ETA: 39s - loss: 1.06 - ETA: 39s - loss: 1.06 - ETA: 38s - loss: 1.07 - ETA: 38s - loss: 1.06 - ETA: 38s - loss: 1.07 - ETA: 37s - loss: 1.07 - ETA: 37s - loss: 1.07 - ETA: 36s - loss: 1.07 - ETA: 36s - loss: 1.07 - ETA: 36s - loss: 1.07 - ETA: 35s - loss: 1.07 - ETA: 35s - loss: 1.07 - ETA: 34s - loss: 1.06 - ETA: 34s - loss: 1.06 - ETA: 34s - loss: 1.06 - ETA: 33s - loss: 1.06 - ETA: 33s - loss: 1.06 - ETA: 32s - loss: 1.07 - ETA: 32s - loss: 1.06 - ETA: 32s - loss: 1.06 - ETA: 31s - loss: 1.06 - ETA: 31s - loss: 1.06 - ETA: 30s - loss: 1.06 - ETA: 30s - loss: 1.06 - ETA: 30s - loss: 1.06 - ETA: 29s - loss: 1.06 - ETA: 29s - loss: 1.06 - ETA: 29s - loss: 1.06 - ETA: 28s - loss: 1.06 - ETA: 28s - loss: 1.06 - ETA: 27s - loss: 1.06 - ETA: 27s - loss: 1.06 - ETA: 27s - loss: 1.06 - ETA: 26s - loss: 1.06 - ETA: 26s - loss: 1.06 - ETA: 25s - loss: 1.06 - ETA: 25s - loss: 1.06 - ETA: 25s - loss: 1.06 - ETA: 24s - loss: 1.06 - ETA: 24s - loss: 1.06 - ETA: 23s - loss: 1.06 - ETA: 23s - loss: 1.06 - ETA: 23s - loss: 1.06 - ETA: 22s - loss: 1.06 - ETA: 22s - loss: 1.06 - ETA: 22s - loss: 1.07 - ETA: 21s - loss: 1.07 - ETA: 21s - loss: 1.07 - ETA: 20s - loss: 1.06 - ETA: 20s - loss: 1.06 - ETA: 20s - loss: 1.06 - ETA: 19s - loss: 1.07 - ETA: 19s - loss: 1.07 - ETA: 18s - loss: 1.06 - ETA: 18s - loss: 1.06 - ETA: 18s - loss: 1.07 - ETA: 17s - loss: 1.06 - ETA: 17s - loss: 1.06 - ETA: 16s - loss: 1.06 - ETA: 16s - loss: 1.06 - ETA: 16s - loss: 1.06 - ETA: 15s - loss: 1.06 - ETA: 15s - loss: 1.07 - ETA: 15s - loss: 1.07 - ETA: 14s - loss: 1.07 - ETA: 14s - loss: 1.07 - ETA: 13s - loss: 1.07 - ETA: 13s - loss: 1.07 - ETA: 13s - loss: 1.07 - ETA: 12s - loss: 1.06 - ETA: 12s - loss: 1.07 - ETA: 11s - loss: 1.07 - ETA: 11s - loss: 1.06 - ETA: 11s - loss: 1.07 - ETA: 10s - loss: 1.07 - ETA: 10s - loss: 1.07 - ETA: 10s - loss: 1.07 - ETA: 9s - loss: 1.0708 - ETA: 9s - loss: 1.071 - ETA: 8s - loss: 1.071 - ETA: 8s - loss: 1.071 - ETA: 8s - loss: 1.070 - ETA: 7s - loss: 1.070 - ETA: 7s - loss: 1.070 - ETA: 6s - loss: 1.070 - ETA: 6s - loss: 1.069 - ETA: 6s - loss: 1.069 - ETA: 5s - loss: 1.068 - ETA: 5s - loss: 1.068 - ETA: 5s - loss: 1.069 - ETA: 4s - loss: 1.069 - ETA: 4s - loss: 1.069 - ETA: 3s - loss: 1.069 - ETA: 3s - loss: 1.068 - ETA: 3s - loss: 1.068 - ETA: 2s - loss: 1.068 - ETA: 2s - loss: 1.067 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.067 - ETA: 1s - loss: 1.068 - ETA: 0s - loss: 1.068 - ETA: 0s - loss: 1.0687####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "\"_We can be able to do the worksheet and then we did the project and then the questions and they are supposed to do the team worksheet and therefore is a team worksheet and then completed the project that we are also a team and we did the team worksheet and do the project and discussed the workshee\n",
      "\n",
      "and all the team worksheet and then discuss the team worksheet and then shared the worksheet and then completed the worksheet and addition to do the worksheet and then completed the project and then they are asked to the project to add the project and they will discuss the team worksheet to do the \n",
      "\n",
      "the project and the team worksheet and adding only that we are not according to do the worksheet and addition to be able to complete the worksheet and stakeholder and then the team worksheet worksheet and when we are also to complete the worksheet and when they will answered the team worksheet and \n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "addition of the team made a project made a week of the part week and each other to do the team completed the worksheet and did not have to complete the constraints to complete the project and they of the constraints are there are all the questions that we have to complete the worksheet with the acc\n",
      "\n",
      "contribution to the question we are so all the team in the worksheet and team worksheet and are the member of all the worksheet on the worksheet to do and stakeholder or not have in the worksheet group that we all made a project I were not needed to the project and some of the worksheet in the begi\n",
      "\n",
      "\"My team worksheet is the project and I have done and were the stakeholder and where we do the questions to do the project and can the project project to complete and project to other and work documents that we are attempting the worksheet and were all the team makes me finish some of the worksheet\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "the Seconuman is tasking or online alabal, about helped us for an\n",
      "\n",
      "cosplays and can complete a there, we get for the project in the team so that all.\"\n",
      "\n",
      "atly different oxpers\n",
      "\n",
      "387/387 [==============================] - 290s 749ms/step - loss: 1.0687\n"
     ]
    }
   ],
   "source": [
    "## The format of the input file is simply one line per document. \n",
    "## When preparing the file, include opening and closing quotes for accurately preprocessing\n",
    "## in the output, the temperature value (0 to 1) refers to the level of creativity\n",
    "\n",
    "## At the end of the training, the model is saved to a file textgenrnn_weights.hdf5\n",
    "\n",
    "textgen = textgenrnn()\n",
    "textgen.train_from_file('./data/reflections.txt', max_length=40, word_level=True, rnn_size=64,  num_epochs=2, dim_embeddings=100, rnn_bidirectional=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now the fun part, to generate some random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support charter of the team worksheet and we shared doc for the time and planning through the terms to do the worksheet and according to the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:10<00:10, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "according to ensure all of the questions and staff of the team worksheet that we also the presentation of the team worksheet and do the end application required and also the question to come to complete the team and I can be able to share the project and will request at the project and then increas\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:30<00:00, 15.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate 1 text document\n",
    "textgen.generate(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate 1 text document starting with a seed setence such as \"My team mates decide to use Google to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My team mates decide to use Google for the team of the worksheet and adding each other to the team worksheet and project. We are doing the worksheet so it will be additing in the project and team does not be able to start answering project each\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(1, prefix=\"My team mates decide to use Google for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise A\n",
    "\n",
    "As with any training of ML/DL models, a lot of work goes into the tuning of the hyperparameters and applying  intuition on what might yield an acceptable results.\n",
    "For the task of generating text, the following parameters should result in different performance of the model.\n",
    "\n",
    "- Are you using training based on word sequence or character sequence?\n",
    "- What is the size of the dimensions? (We admit that don't know if the module textgenrnn is using word2vec or other variations. This is not documentation)\n",
    "- Whether you are training in the forward direction or using bi-directional network?\n",
    "\n",
    "\n",
    "#### Your task: \n",
    "Change the input parameters of the method train_from_file() to try out the different values of hyperparameters\n",
    "- word_level=True, word_level=False\n",
    "- dim_embeddings=100, dim_embeddings=50, dim_embeddings=150\n",
    "- rnn_bidirectional=False, rnn_bidirectional=True\n",
    "\n",
    "The, retrain the model and generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answers\n",
    "textgen.train_from_file('./data/reflections.txt', max_length=40, word_level=True, rnn_size=64,  num_epochs=2, dim_embeddings=100, rnn_bidirectional=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise\n",
    "\n",
    "Use a different source text such as songs or quotes or product review and see what AI can generate for you !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
