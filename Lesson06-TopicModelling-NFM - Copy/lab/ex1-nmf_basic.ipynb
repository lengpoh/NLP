{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling with NMF\n",
    "#### A really trivial exercise to warm up with NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up\n",
    "- For simplicity , we prepare the corpus to be a list of documents.\n",
    "- In the ideal case, you should process the input text to remove stop words, perform lemmization etc\n",
    "- Prepare vectorization using TFID of the input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"the cat eat rice\", \n",
    "             \"secret message emailed by agent cat\", \n",
    "             \"today must go shopping for cat food\"]\n",
    "tf_vectorizer = TfidfVectorizer( stop_words='english')\n",
    "tf_vectorized_documents = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the extracted bag-of-word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agent',\n",
       " 'cat',\n",
       " 'eat',\n",
       " 'emailed',\n",
       " 'food',\n",
       " 'message',\n",
       " 'rice',\n",
       " 'secret',\n",
       " 'shopping',\n",
       " 'today']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. What are the stop words in the corpus?\n",
    "2. Are the stop words in the feature list?\n",
    "2. If so, how where the stop words removed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the term document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.652490884512534\n",
      "  (0, 2)\t0.652490884512534\n",
      "  (0, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.479527938028855\n",
      "  (1, 3)\t0.479527938028855\n",
      "  (1, 5)\t0.479527938028855\n",
      "  (1, 7)\t0.479527938028855\n",
      "  (1, 1)\t0.2832169249871526\n",
      "  (2, 4)\t0.546454011634009\n",
      "  (2, 8)\t0.546454011634009\n",
      "  (2, 9)\t0.546454011634009\n",
      "  (2, 1)\t0.3227445421804912\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and transform the vectorized document into NMF model\n",
    "- To align with convention used in the note, we will use A (or V), W, H\n",
    "-  W * H = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=5, init='random',   random_state=0 )\n",
    "A = tf_vectorized_documents\n",
    "W = nmf_model.fit_transform(A)\n",
    "H = nmf_model.components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view the lda model parameters\n",
    "- There are many parameters to tune. We are only interested into the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='random', l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=0, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explore contents of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)     # controls the precision of the np output inherent in W and H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000e+00 1.3293e+00 0.0000e+00 0.0000e+00 9.5947e-02]\n",
      " [7.9892e-01 0.0000e+00 1.4538e-08 2.8014e-01 0.0000e+00]\n",
      " [0.0000e+00 1.4770e-04 5.2578e-01 5.7715e-06 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explore contents of H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4642 0.3434 0.     0.4074 0.     0.4814 0.     0.4702 0.     0.    ]\n",
      " [0.     0.2899 0.3642 0.     0.     0.     0.3837 0.     0.     0.    ]\n",
      " [0.     0.6138 0.     0.     1.0393 0.     0.     0.     1.0393 1.0393]\n",
      " [0.388  0.0317 0.     0.55   0.     0.3389 0.     0.3708 0.     0.    ]\n",
      " [0.     0.     1.7546 0.     0.     0.     1.4841 0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. How many documents are there in the original corpus?\n",
    "2. What is the number of terms that are extracted?\n",
    "3. In W, how many items(list) are there in the array? \n",
    "4. In H, how many items(list) are there in the array? What does the items refer to\n",
    "5. Fill in the blank: Row 3 (index 2) of W refers to the document _______________________\n",
    "6. Fill in the blank: The predominant topic related to Row 3 is topic ___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "1. Change the number of topics (e.g. set n_components = 6) and run the notebook.\n",
    "2. Describe (in terms of dimensions) how has W and  H change.\n",
    "3. Increase documents with 2 more sentences. Describe the changes in W, H and A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
