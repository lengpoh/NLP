{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Selection for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the neccessary libraries\n",
    "- LDA works only with bag of words approach only\n",
    "- Regular expressions re, gensim and spacy are used to process texts. \n",
    "- PyLDAvis and matplotlib for visualization and numpy\n",
    "- Pandas for manipulating and viewing data in tabular format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### display_topic() is a commonly used function to display topics and related terms\n",
    "- model - the lda model\n",
    "- feature_names - the features names\n",
    "- no_topc_words - how many terms to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "- Clean_documents() is a function to perform data cleansing for a raw document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_documents(raw_document):\n",
    "    # placeholder: Write data preparation codes here\n",
    "    document = raw_document\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the section to modify if you have other sources\n",
    "- Load in the documents from its source\n",
    "- The LDA topic model algorithm requires a document word matrix as the main input.\n",
    "- Vectorise the document using count vectorizing\n",
    "- LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = clean_documents(dataset.data)\n",
    "\n",
    "no_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf_vectorized_documents = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Do you know you have a good  model performance\n",
    "- One way is to use with perplexity and log-likelihood\n",
    "- A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good\n",
    "- The only drawback is tha it doesnâ€™t consider the context and semantic associations between words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to start by pre-specifying an initial range of \"sensible\" values:\n",
    "- Then apply LDA for all possible topic size k\n",
    "- Calculate the log-likeliihood and perplexity at the same time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "\n",
    "The codes segment below implements a loop to perform modelling for a range of values of k (components)\n",
    "Use you intuition, set some boundaries for min and max number topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin= 15\n",
    "kmax = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:\n",
    "Initiate the LDA model and call the relevant methods in the lda object to return to log_likehood and perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LDA for k=15 ...\n",
      "Applying LDA for k=16 ...\n",
      "Applying LDA for k=17 ...\n",
      "Applying LDA for k=18 ...\n",
      "Applying LDA for k=19 ...\n",
      "Applying LDA for k=20 ...\n",
      "Applying LDA for k=21 ...\n",
      "Applying LDA for k=22 ...\n",
      "Applying LDA for k=23 ...\n",
      "Applying LDA for k=24 ...\n",
      "Applying LDA for k=25 ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "topic_models = []\n",
    "# try each value of k\n",
    "for k in range(kmin,kmax+1):\n",
    "    print(\"Applying LDA for k=%d ...\" % k )\n",
    "    lda_model = LatentDirichletAllocation(n_components=k, max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "    lda_output = lda_model.fit_transform(tf_vectorized_documents)  \n",
    "    log_likelihood = lda_model.score(tf_vectorized_documents)\n",
    "    perplexity = lda_model.perplexity(tf_vectorized_documents)\n",
    "    topic_models.append( (k,lda_model,lda_output, log_likelihood, perplexity) ) # store for later\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prints the number of topics and corresponding perplexity and log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k topics :  15, Log Likelihood : -3059155.03   Perplexity : 259.51\n",
      "k topics :  16, Log Likelihood : -3057436.03   Perplexity : 258.70\n",
      "k topics :  17, Log Likelihood : -3054443.24   Perplexity : 257.29\n",
      "k topics :  18, Log Likelihood : -3051152.22   Perplexity : 255.76\n",
      "k topics :  19, Log Likelihood : -3052920.90   Perplexity : 256.58\n",
      "k topics :  20, Log Likelihood : -3049240.73   Perplexity : 254.87\n",
      "k topics :  21, Log Likelihood : -3048468.07   Perplexity : 254.52\n",
      "k topics :  22, Log Likelihood : -3047531.30   Perplexity : 254.08\n",
      "k topics :  23, Log Likelihood : -3055736.92   Perplexity : 257.90\n",
      "k topics :  24, Log Likelihood : -3051220.08   Perplexity : 255.79\n",
      "k topics :  25, Log Likelihood : -3054335.57   Perplexity : 257.24\n"
     ]
    }
   ],
   "source": [
    "for model in topic_models:\n",
    "  print(\"k topics : % 2d, Log Likelihood : % 5.2f   Perplexity : %5.2f\" %(model[0], model[3], model[4]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Based on results, how should  the Log Likelihood  & Perplexity be interpreted?\n",
    "2. Which topic number provides the best 'optimal number' of topic?\n",
    "3. Would this 'best' numbers be the final answer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "government gun state law people states right rights guns control\n",
      "Topic 1:\n",
      "new national public private health use administration years service research\n",
      "Topic 2:\n",
      "greek body right sound left order david pro mentioned cross\n",
      "Topic 3:\n",
      "key chip encryption keys clipper use des algorithm government security\n",
      "Topic 4:\n",
      "10 15 11 12 25 14 17 16 20 13\n",
      "Topic 5:\n",
      "better does case point use way question problem don think\n",
      "Topic 6:\n",
      "windows thanks window use using dos does help problem know\n",
      "Topic 7:\n",
      "information university 1993 general medical water april air new page\n",
      "Topic 8:\n",
      "just don like know think ve people time say good\n",
      "Topic 9:\n",
      "00 car power 50 speed cars 000 engine new used\n",
      "Topic 10:\n",
      "edu com available ftp mail list pub information software version\n",
      "Topic 11:\n",
      "game team year play games season hockey league players win\n",
      "Topic 12:\n",
      "god jesus people believe christian bible does life church say\n",
      "Topic 13:\n",
      "mr people think don group going president know stephanopoulos yes\n",
      "Topic 14:\n",
      "drive card db scsi disk mac hard memory apple drivers\n",
      "Topic 15:\n",
      "price bike offer sale buy interested mail sell good like\n",
      "Topic 16:\n",
      "ax max b8f g9v a86 pl 145 1d9 0t 34u\n",
      "Topic 17:\n",
      "space nasa science program data earth launch build satellite scientific\n",
      "Topic 18:\n",
      "said people armenian israel armenians jews war turkish israeli women\n",
      "Topic 19:\n",
      "day current news world week food black media times local\n",
      "Topic 20:\n",
      "file entry book read edu article subject line books section\n",
      "Topic 21:\n",
      "output image color graphics input display bit ibm jpeg pc\n"
     ]
    }
   ],
   "source": [
    "### Answer:\n",
    "no_top_words = 10\n",
    "best_k = 22 \n",
    "best_lda_model = topic_models[7][1]\n",
    "display_topics(best_lda_model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optional Exercises:\n",
    "1. Retrieve the best model\n",
    "2. Display the topics\n",
    "3. Check if the terms make sense\n",
    "4. Create a Panda dataframe to show the relationship between terms, topics and documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "- www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
