{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Word2Vec to Solve Document Classification Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement: This notebook is provided by Intel AI Developer Program\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Word2Vec are a representation of vocabulary and its features. \n",
    "Once we have vectors for examples, we can perform Machine Learning (both supervised and unsupervised).\n",
    "We can use it to solve classification problem.\n",
    "\n",
    "In this exercise, we be using a well-known text datasets to explore the capabilities of Word2Vec:\n",
    "- [20 Newsgroups Dataset](http://qwone.com/~jason/20Newsgroups/): Famous text classification dataset from user discussion forums with 20 classes, including...\n",
    "  \n",
    "To perform our tasks, we will both derive our own **word vectors** from the data as well as borrow Google's massive set of word vectors trained on the web ([Google Vectors]())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:54:14.154063Z",
     "start_time": "2017-07-27T21:54:09.408786Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLP tools\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# Data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Necessary for adding accessory_functions module to path\n",
    "import os, sys\n",
    "lib_path = os.path.abspath(os.path.join('..', '..'))\n",
    "sys.path.append(lib_path)\n",
    "from accessory_functions import google_vec_file, nltk_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to derive document vectors\n",
    "Consider the case of document classification.  From Word2Vec we have vectors for words, but for our examples, we need classify are documents.  How do we get vectors representation for whole documents? The most common answer is to take an average of all the word vectors in a document.  Let's try that with our sample data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will return the document vectors which will be used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take a document as a list of words and the word2Vec model.\n",
    "# The function will check if the word vector exists. If so, the word is added to good_words list.\n",
    "# Finally, the mean of vectors for all words is returned.\n",
    "# What is eventually returned is the document vector.\n",
    "\n",
    "def get_doc_vec(words, model):\n",
    "    good_words = []   #good_words is a list of words where the word vectors are available in the model\n",
    "    for word in words:\n",
    "        # Words not in the original model will fail\n",
    "        try:\n",
    "            if model[word] is not None:   # None is when the word vector isn't available\n",
    "                good_words.append(word)\n",
    "        except:\n",
    "            continue\n",
    "    # If no words are in the original model\n",
    "    if len(good_words) == 0:\n",
    "        return None\n",
    "    # Return the mean of the vectors for words found in good_woods\n",
    "    # ref to https://www.geeksforgeeks.org/numpy-mean-in-python/ for documentation\n",
    "    return model[good_words].mean(axis=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Prepare The Data\n",
    "\n",
    "We will be using a portion of a data set containing approximately 20,000 posts partitioned evenly across 20 different newsgroups. This data set is quite famous. We will be using a sample of this data set, containing 5 topics and about 3,000 posts. We will need to load in the data.\n",
    "\n",
    "The cell below loads  the input data into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from accessory_functions import preprocess_series_text, nltk_path\n",
    "\n",
    "topic_list = ['sci.space', 'comp.sys.mac.hardware', 'rec.autos',\n",
    "              'rec.sport.baseball', 'sci.med']\n",
    "\n",
    " \n",
    "# Retrieve the data into a DataFrame\n",
    "# #The data is a dictionary of key 'data', and a list containing string of the text\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, data_home='./data',\n",
    "                             categories=topic_list,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below  preprocess the input data into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2956\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>otoh u get lucky unplugged replugged scsi adb ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>yes everyone else may wonder fred well would o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>umm perhaps could explain right talk</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>like alomar like differ opinion city likely po...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wow know uranus long way think far away</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>outbreak chronic mono like entity originally c...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>couple question multimedia set anybody phone f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>sure dietician date crohn ulcerative colitis p...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>seek recommendation vendor networkable fax wou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>mlb standing score friday april include yester...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  otoh u get lucky unplugged replugged scsi adb ...      0\n",
       "1  yes everyone else may wonder fred well would o...      4\n",
       "2               umm perhaps could explain right talk      4\n",
       "3  like alomar like differ opinion city likely po...      2\n",
       "4            wow know uranus long way think far away      4\n",
       "5  outbreak chronic mono like entity originally c...      3\n",
       "6  couple question multimedia set anybody phone f...      0\n",
       "7  sure dietician date crohn ulcerative colitis p...      3\n",
       "8  seek recommendation vendor networkable fax wou...      0\n",
       "9  mlb standing score friday april include yester...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_data = pd.DataFrame(dataset['data'], columns=['text'])\n",
    "ng_data['label'] = dataset['target']\n",
    "\n",
    "# Preprocess\n",
    "# The function preprocess_series_text() defined in the accesssory_functions.py file\n",
    "ng_data['text'] = preprocess_series_text(ng_data.text, nltk_path=nltk_path)\n",
    "\n",
    "print(len(ng_data))\n",
    "ng_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Word2Vec model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this section, you will train a custom word2Vec  based on sentences from documents.\n",
    "\n",
    "#### Train a Word2Vec model to generate word vectors from the 20 Newsgroups data. \n",
    "* Split documents into sentences\n",
    "* Split each sentences into a list of words since gensim requires the documents to be represented as a list of sentences in tokens to train Word2Vec\n",
    "* Instantiate a new Word2Vec object\n",
    "* Use your custom Word2Vec model to generate document vectors from these word vectors.\n",
    "* Combine these vectors with the 20 Newsgroups class labels to create a DataFrame for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6005, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Generate sentences for training word2vec\n",
    "sentences = ng_data.text.str.split()  ## Generate sentences for training word2vec\n",
    "# Train a Word2Vec model (ng_model ==> newsgroup model)\n",
    "ng_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "# check the shape of the custom ng_model\n",
    "print(ng_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('full', 0.9991823434829712),\n",
       " ('plant', 0.9991742372512817),\n",
       " ('self', 0.9991512298583984),\n",
       " ('rate', 0.999147355556488),\n",
       " ('fluid', 0.999129056930542),\n",
       " ('consideration', 0.9991122484207153),\n",
       " ('burst', 0.9991054534912109),\n",
       " ('country', 0.9991052150726318),\n",
       " ('tax', 0.9991040825843811),\n",
       " ('gun', 0.9991024732589722)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Did it work? Check out similar words to baseball, airplane etc\n",
    "ng_model.wv.most_similar('airplane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Derive Document Vectors from Word Vectors\n",
    "Now that the custom Word2Vec model is created, we can use it, together with the document, to call the function get_doc_vec(defined earlier) to return a vector that represents the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the new group dataframe for the Google word vector\n",
    "ng_data1 = ng_data.copy()\n",
    "\n",
    "# Retrieve the document vectors based on newsgroup word vectors\n",
    "ng_vecs = ng_data1.text.str.split().map(lambda x: get_doc_vec(x, ng_model.wv))\n",
    "\n",
    "# Add to dataframe\n",
    "ng_data1['vecs'] = ng_vecs\n",
    "\n",
    "# Drop the bad docs\n",
    "ng_data1 = ng_data1.dropna()\n",
    "\n",
    "# Create a Numpy array of the document vectors\n",
    "ng_np_vecs = np.zeros((len(ng_data1), 100))\n",
    "for i, vec in enumerate(ng_data1.vecs):\n",
    "    ng_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "ng_w2v_data = pd.concat([ng_data1.reset_index().label, pd.DataFrame(ng_np_vecs)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>vecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>otoh u get lucky unplugged replugged scsi adb ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.3021288, 0.66113096, -0.48783496, 0.263959...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>yes everyone else may wonder fred well would o...</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.32507056, 0.6875524, -0.47469854, 0.254792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>umm perhaps could explain right talk</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.36925447, 0.79559404, -0.5978705, 0.325746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>like alomar like differ opinion city likely po...</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.32073867, 0.6705939, -0.5382954, 0.2905089...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wow know uranus long way think far away</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.3174214, 0.67041767, -0.5557123, 0.2942629...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  otoh u get lucky unplugged replugged scsi adb ...      0   \n",
       "1  yes everyone else may wonder fred well would o...      4   \n",
       "2               umm perhaps could explain right talk      4   \n",
       "3  like alomar like differ opinion city likely po...      2   \n",
       "4            wow know uranus long way think far away      4   \n",
       "\n",
       "                                                vecs  \n",
       "0  [-0.3021288, 0.66113096, -0.48783496, 0.263959...  \n",
       "1  [-0.32507056, 0.6875524, -0.47469854, 0.254792...  \n",
       "2  [-0.36925447, 0.79559404, -0.5978705, 0.325746...  \n",
       "3  [-0.32073867, 0.6705939, -0.5382954, 0.2905089...  \n",
       "4  [-0.3174214, 0.67041767, -0.5557123, 0.2942629...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained 20 Newsgroups Classifier\n",
    "Now that we have the document vector setup, we can start to use classification algorithms available in sklearn to built classification models based on documents vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training a Classifier with our own trained vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_ng1 = ng_w2v_data.iloc[:, 1:]\n",
    "y_ng1 = ng_w2v_data.label\n",
    "X_train_ng1, X_test_ng1, y_train_ng1, y_test_ng1 = train_test_split(X_ng1, y_ng1, test_size=0.3)\n",
    "\n",
    "# Train a KNN or Logistic Regression classifier\n",
    "est = KNeighborsClassifier(algorithm='brute', metric='cosine')\n",
    "# est = LogisticRegression()\n",
    "est.fit(X_train_ng1, y_train_ng1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score for the model is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6619718309859155"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.score(X_test_ng1, y_test_ng1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Google Word2Vec¶\n",
    "\n",
    "In this section, you will repeat the earlier steps,  but this time using Google's pretrained model and then  perform a classification task.\n",
    "\n",
    "* Use the Google vectors to generate document vectors for the 20 Newsgroups data.\n",
    "* Combine these vectors with the 20 Newsgroups class labels to create a DataFrame for classification.\n",
    "* Train a classification model for these five 20 Newsgroups classes and evaluate its performance.  \n",
    "  **Hint**: Try a K-Nearest Neighbors Model.\n",
    "\n",
    "Note the performance of the Google vectors vs your own Word2Vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading Google Word2Vec Vectors\n",
    "* Load the Google vectors into an object `google_model` using `gensim` \n",
    "* This step will take awhile, as it has to load 3 million vectors into the appropriate Word2Vec format.\n",
    "* Google's model contains an extensive vocabulary.\n",
    "* Confirm that you have 3 million vectors of length 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Google vectors\n",
    "google_vec_file = \"./data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Derive Document Vectors from Word Vectors\n",
    "Now that the Google word2vec model is loaded, we can use it, together with the document, to call the function get_doc_vec(defined earlier) to return a vector that represents the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the spam dataframe for the Google work\n",
    "ng_data2 = ng_data.copy()\n",
    "\n",
    "# Retrieve the document vectors based on google word vectors\n",
    "ng_google_vecs = ng_data2.text.str.split().map(lambda x: get_doc_vec(x, google_model))\n",
    "\n",
    "# Add to dataframe\n",
    "ng_data2['vecs'] = ng_google_vecs\n",
    "\n",
    "# Drop the bad docs\n",
    "ng_data2 = ng_data2.dropna()\n",
    "\n",
    "# Create a Numpy array of the document vectors\n",
    "ng_np_vecs = np.zeros((len(ng_data2), 300))\n",
    "for i, vec in enumerate(ng_data2.vecs):\n",
    "    ng_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "ng_google_data = pd.concat([ng_data2.reset_index().label, pd.DataFrame(ng_np_vecs)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Word2Vec 20 Newsgroups Classifier\n",
    "Now that we have the document vector setup, we can start to use classification algorithms available in sklearn to built classification models based on documents vectors\n",
    "* Train a classification model for these five 20 Newsgroups classes and evaluate its performance. Hint: Try a K-Nearest Neighbors Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training a Classifier with Google's vectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data\n",
    "X_ng2 = ng_google_data.iloc[:, 1:]\n",
    "y_ng2 = ng_google_data.label\n",
    "X_train_ng2, X_test_ng2, y_train_ng2, y_test_ng2 = train_test_split(X_ng2, y_ng2, test_size=0.3)\n",
    "\n",
    "# Train a KNN or Logistic Regression classifier\n",
    "est = KNeighborsClassifier(algorithm='brute', metric='cosine')\n",
    "# est = LogisticRegression()\n",
    "est.fit(X_train_ng2, y_train_ng2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.score(X_test_ng2, y_test_ng2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "Compare the score for the Google Word2Ved model and the custom Word2Vec model.L\n",
    "Which one performs better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A\n",
    "\n",
    "So far, you have only performed classification for 5 newsgroup. \n",
    "Repeat the training process, but this time increases to all 20 newsgroup. \n",
    "1. Modify the code to include all 20  newsgroup\n",
    "1. Has the score for the custom word vector improve, got worse or remain the same?\n",
    "2. Has the score for the Google pretrained improve, got worse or remain the same?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your codes\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B\n",
    "According to Mikolov, Skip Gram works well with small amount of data and is found to represent rare words well. On the other hand, CBOW is faster and has better representations for more frequent words.\n",
    "1. What was the default model used when instantiating our custom Word2Vec? (Hint: look the hyperparameter sg)\n",
    "2. Modify the codes to use CBOW to instantiate the model.\n",
    "3. Repeat the classification training.\n",
    "4. Discuss if the result is consistent with Mikolov.\n",
    "\n",
    "Hint: Refer to documenation at https://radimrehurek.com/gensim/models/word2vec.html for sg parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
