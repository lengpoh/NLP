{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "- We will borrow Google's massive set of word vectors trained on the web ([Google Vectors]()).\n",
    "- Gensim is used toe explore the basic semantic queries that you can perform with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:54:14.154063Z",
     "start_time": "2017-07-27T21:54:09.408786Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLP tools\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# Data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:57:16.564833Z",
     "start_time": "2017-07-27T21:54:14.156571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the Google vectors\n",
    "\n",
    "#google_vec_file = \"/Users/tanpohkeam/Downloads/GoogleNews-vectors-negative300.bin.gz\"\n",
    "google_vec_file = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google's model contains an extensive vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare similarities between words.\n",
    "We can use the similarity() method to compare words for similarity\n",
    "- Let us compare the same pair of words. We should get a similarity score of 1.0\n",
    "- Also, let us compare an odd couple of words. We should get a similarity score of close to 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.similarity('ocean', 'ocean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19793196"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.similarity('shark', 'volcano')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A:\n",
    "- For the following parts of words, use your human intuition to rank the level of similar between the pair of words. \n",
    "- Pair 1: (ocean and sea)\n",
    "- Pair 2: (ocean and river)\n",
    "- Pair 3: (river and salmon)\n",
    "- Pair 4: (ocean and mountain)\n",
    "- Pair 5: (frog and snail)\n",
    "\n",
    "- Then write the codes to get the similarity score. Was the result of Word2Vec consistent with human knowledge?\n",
    "\n",
    "Note: There is no known threshold for what consistuents a threshold for similarity.\n",
    "What is interesting to know is that the Word2Vec is able to learn the vector representation that results in similiarity score that is close to human judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27277806"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your codes\n",
    "w1 = 'ocean'\n",
    "#w2 = 'sea'\n",
    "w2 = 'mountain'\n",
    "google_model.similarity(w1, w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B\n",
    "Let see how the pretrained model handles names of personalities (real or fake) who frequents the news\n",
    "\n",
    "- Find the similarity all pair combination between 'Lee_Kuan_Yew',Mahathir_Mohamad, Clinton , Bush, Spiderman, Superman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4515921"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your codes\n",
    "w1 = 'Superman'\n",
    "#w2 = 'sea'\n",
    "w2 = 'Joker'\n",
    "google_model.similarity(w1, w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find Similar words\n",
    "\n",
    "Word2Vec method similar_by_word() can be used to return a list of similar words.\n",
    "Let us try to find some similar words for broom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brooms', 0.7050667405128479),\n",
       " ('broomstick', 0.6114087104797363),\n",
       " ('dustpan', 0.5667167901992798),\n",
       " ('fly_swatter', 0.5471036434173584),\n",
       " ('whisk_broom', 0.5430972576141357),\n",
       " ('dustpans', 0.533984899520874),\n",
       " ('wand', 0.531521201133728),\n",
       " ('shovel', 0.5279634594917297),\n",
       " ('pitchfork', 0.524329662322998),\n",
       " ('mattock', 0.521378755569458)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.similar_by_word('broom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C:\n",
    "- Suppose you are writing an article and find that you are using the word \"happy\" too often. You like to use alternative words that can replace \"happy\". State 5 words that comes to mind?\n",
    "\n",
    "- Ask Word2Vec to suggest some replacement words. Are the returned words suitable as replacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glad', 0.7408890724182129),\n",
       " ('pleased', 0.6632170677185059),\n",
       " ('ecstatic', 0.6626912355422974),\n",
       " ('overjoyed', 0.6599286794662476),\n",
       " ('thrilled', 0.6514049768447876),\n",
       " ('satisfied', 0.6437948942184448),\n",
       " ('proud', 0.636042058467865),\n",
       " ('delighted', 0.6272379159927368),\n",
       " ('disappointed', 0.6269949674606323),\n",
       " ('excited', 0.6247665882110596)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "google_model.similar_by_word('happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar words with some negative examples\n",
    "We can also find-tune finding similar words by providing dissimilar as  example of negative words.\n",
    "In the first example, Word2Vec returns countries in Europe.\n",
    "In the second example, Word2Vec returns countries in Asia.\n",
    "In the first example, Word2Vec does NOT return countries in Europe, but rather terms associated with the terms in the positive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sweden', 0.766769528388977),\n",
       " ('Switzerland', 0.7535009980201721),\n",
       " ('Denmark', 0.7359017729759216),\n",
       " ('Netherlands', 0.7167849540710449),\n",
       " ('Belgium', 0.703701376914978),\n",
       " ('Hungary', 0.6940024495124817),\n",
       " ('Finland', 0.6854315996170044),\n",
       " ('Italy', 0.6712003946304321),\n",
       " ('Slovakia', 0.6686522960662842),\n",
       " ('Croatia', 0.6683070659637451)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 1:\n",
    "google_model.most_similar(positive=['Norway', 'Spain', 'Germany', \"Austria\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('South_Korea', 0.7823339104652405),\n",
       " ('Thailand', 0.7040652632713318),\n",
       " ('Southeast_Asia', 0.69697105884552),\n",
       " ('Viet_Nam', 0.6925818920135498),\n",
       " ('China', 0.6915749311447144),\n",
       " ('Taiwan', 0.6532434225082397),\n",
       " ('Philippines', 0.6405261158943176),\n",
       " ('Malaysia', 0.6370214819908142),\n",
       " ('Cambodia', 0.6269365549087524),\n",
       " ('Asia', 0.6238888502120972)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 2:\n",
    "google_model.most_similar(positive=['Korea', 'Japan', 'Vietnam', \"Indonesia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Austrians', 0.5269070863723755),\n",
       " ('Austrian', 0.5137249231338501),\n",
       " ('Andrea_Rothfuss', 0.4956299066543579),\n",
       " ('Juergen', 0.49309805035591125),\n",
       " ('German', 0.49204903841018677),\n",
       " ('Andreas', 0.49074259400367737),\n",
       " ('Baden_Wurttemberg', 0.4817953109741211),\n",
       " ('MÃ¼ller', 0.48091599345207214),\n",
       " ('Holger', 0.4781285226345062),\n",
       " ('Switzerland', 0.4780736267566681)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 3:\n",
    "google_model.most_similar(positive=['Norway', 'Spain', 'Germany', 'Austria'], negative=['Sweden','Italy', 'England'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D:\n",
    "Run the codes below. Describe the 'similar' words that were returned by Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('South_Korea', 0.5299240946769714),\n",
       " ('Korean', 0.4800219237804413),\n",
       " ('CAMP_HUMPHREYS_South', 0.45830661058425903),\n",
       " ('kamikaze_pilots', 0.44935142993927),\n",
       " ('Japanese', 0.4464004337787628),\n",
       " ('Vietnam_War', 0.4360562860965729),\n",
       " ('World_War_II', 0.4360520541667938),\n",
       " ('WWII', 0.4276370108127594),\n",
       " ('Seoul', 0.42717018723487854),\n",
       " ('Coldest_War', 0.4266488254070282)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.most_similar(positive=['Korea', 'Japan', 'Vietnam', \"Indonesia\"], negative = ['Thailand', 'China', 'Brunei'])\n",
    "\n",
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E:\n",
    "- Find similar words to 'Norway', 'Spain', 'Egypt, 'Norway', 'Spain', 'Egypt', 'Thailand'\n",
    "- Repeat the above, but see if you can exclude countries from Europe from been listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n",
    "google_model.most_similar(positive=['Norway', 'Spain', 'Germany', 'Austria'], negative=['Sweden','Italy', 'England'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
