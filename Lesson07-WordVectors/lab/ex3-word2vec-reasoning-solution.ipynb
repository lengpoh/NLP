{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec  Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement: This notebook is adapted from Intel AI Developer Program\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "- We will borrow Google's massive set of word vectors trained on the web ([Google Vectors]()).\n",
    "- Gensim is used toe explore the basic semantic queries that you can perform with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:54:14.154063Z",
     "start_time": "2017-07-27T21:54:09.408786Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLP tools\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# Data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State the path of google pretrained vectors. Note that this is a huge file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "google_vec_file = \"/Users/tanpohkeam/Downloads/GoogleNews-vectors-negative300.bin.gz\"\n",
    "# Load the Google vectors\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the words are vectorized, you can effectively perform algebra manipulation on it.\n",
    "Depending on the operations on, you will get a new vector that resides on a different part of the vector space.\n",
    "The results may just surprise you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can perform some algebra to find out similar words \n",
    "* For example B, C, D are some valid words\n",
    "* We find their word vectors, and add up to get a  new vector (A). Note that the Vector A may not be present in the model, so we can only find vectors that are similar to A\n",
    "* Finally, we find words that are nearest to the new vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('King', 0.7780153751373291),\n",
       " ('Queen', 0.55495285987854),\n",
       " ('Oprah_BFF_Gayle', 0.4718775153160095),\n",
       " ('Princess', 0.464548259973526),\n",
       " ('Geoffrey_Rush_Exit', 0.46232524514198303)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = google_model.word_vec('King')\n",
    "C = google_model.word_vec('man')\n",
    "D = google_model.word_vec('woman')\n",
    "\n",
    "# example: King - Man + Woman\n",
    "A = B + D - C\n",
    "google_model.most_similar(positive=[A], topn=5)\n",
    "\n",
    "# The results is interesting, as it identifies Queen and Princess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above A = B + D - C can also be written as\n",
    "google_model.most_similar(positive=[B, D], negative=[C],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('King', 0.7780153751373291),\n",
       " ('Queen', 0.55495285987854),\n",
       " ('Oprah_BFF_Gayle', 0.4718775153160095),\n",
       " ('Princess', 0.4645482301712036),\n",
       " ('Geoffrey_Rush_Exit', 0.46232521533966064)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.most_similar(positive=[B, D], negative=[C],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when you add up the countries of South East Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Malaysia', 0.9048078060150146),\n",
       " ('Indonesia', 0.8643579483032227),\n",
       " ('Thailand', 0.8491654992103577),\n",
       " ('Singapore', 0.8213659524917603),\n",
       " ('Malaysian', 0.743881106376648),\n",
       " ('Southeast_Asia', 0.7242699861526489),\n",
       " ('Indonesian', 0.690682053565979),\n",
       " ('Kuala_Lumpur', 0.6743208169937134),\n",
       " ('Malaysias', 0.662969708442688),\n",
       " ('Brunei', 0.6522230505943298)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = google_model.word_vec('Singapore') \n",
    "C = google_model.word_vec('Malaysia') \n",
    "D = google_model.word_vec('Thailand') \n",
    "E = google_model.word_vec('Indonesia') \n",
    "A = B + C + D + E\n",
    "google_model.most_similar(positive=[A], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A:\n",
    "* Try the following reasoning to see if you can get meaningful results\n",
    "- ham + bread\n",
    "- Chicken + Duck + Oven\n",
    "- milk + child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ham', 0.8338900804519653),\n",
       " ('bread', 0.8255820274353027),\n",
       " ('vegetable_casserole', 0.6764018535614014),\n",
       " ('french_bread', 0.6642782688140869),\n",
       " ('roast_beef', 0.6588861346244812)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Answer\n",
    "A = google_model.word_vec('ham') + google_model.word_vec('bread')  \n",
    "google_model.most_similar(positive=[A], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('oven', 0.7615751028060913),\n",
       " ('chicken', 0.7566594481468201),\n",
       " ('duck', 0.6915632486343384),\n",
       " ('Pour_marinade', 0.6730190515518188),\n",
       " ('drumettes', 0.6670605540275574)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Answer\n",
    "A = google_model.word_vec('chicken') + google_model.word_vec('duck') + google_model.word_vec('oven') \n",
    "google_model.most_similar(positive=[A], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('milk', 0.8140528202056885),\n",
       " ('child', 0.7395879030227661),\n",
       " ('baby', 0.6311920285224915),\n",
       " ('infant', 0.6166431307792664),\n",
       " ('camels_Nancy_Riegler', 0.6104888319969177)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Answer\n",
    "A = google_model.word_vec('milk') + google_model.word_vec('child')\n",
    "google_model.most_similar(positive=[A], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B:\n",
    "- Can we find perhaps ask Word2Vec to find a man who has walked on the moon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moon', 0.7205320596694946),\n",
       " ('walk', 0.6430635452270508),\n",
       " ('Feustel_replied', 0.5959716439247131),\n",
       " ('man', 0.5784651041030884),\n",
       " ('Today_waxing_Virgo', 0.5584660768508911),\n",
       " ('walking', 0.5477094650268555),\n",
       " ('Astronaut_Eugene_Cernan', 0.543879508972168),\n",
       " ('wolf_howling', 0.5324627161026001),\n",
       " ('Armstrong_Eugene_Cernan', 0.5185102224349976),\n",
       " ('astronaut_Gene_Cernan', 0.5145226120948792)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Answer\n",
    "B = google_model.word_vec('man') \n",
    "C =  google_model.word_vec('walk') \n",
    "D = google_model.word_vec('moon')\n",
    "\n",
    "A = B + C + D\n",
    "google_model.similar_by_word(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR CLASS DEMO ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Prime_Minister', 0.8192539215087891),\n",
       " ('Singapore', 0.7209081053733826),\n",
       " ('Prime_Minster', 0.7182164192199707),\n",
       " ('prime_minister', 0.6842101812362671),\n",
       " ('BG_Yeo', 0.6375665664672852),\n",
       " ('Prime_MInister', 0.6349261999130249),\n",
       " ('Deputy_Prime_Minister', 0.6288642287254333),\n",
       " ('Prime_Minsiter', 0.6287922859191895),\n",
       " ('Prime_Mininster', 0.6208765506744385),\n",
       " ('Singaporean_counterpart', 0.6199405193328857)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = google_model.word_vec('Prime_Minister')\n",
    "B = google_model.word_vec('Singapore')\n",
    "\n",
    "google_model.similar_by_word( A + B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thailand', 0.8176553249359131),\n",
       " ('Malaysia', 0.6191744208335876),\n",
       " ('Indonesia', 0.5976786613464355),\n",
       " ('Cambodia', 0.5769325494766235),\n",
       " ('Southeast_Asia', 0.5457445383071899),\n",
       " ('BT_##.####_##.####', 0.5402755737304688),\n",
       " ('Thai', 0.5289890170097351),\n",
       " ('Thailands', 0.5230374932289124),\n",
       " ('Philippines', 0.5194821357727051),\n",
       " ('Thais', 0.5171231627464294)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = google_model.word_vec('Japan')\n",
    "B = google_model.word_vec('Tokyo')\n",
    "C = google_model.word_vec('')\n",
    "\n",
    "google_model.similar_by_word( A - B + C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
