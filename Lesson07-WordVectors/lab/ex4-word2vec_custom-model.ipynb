{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "If you are working on a domain specific NLP problem, using pre-trained models\n",
    "may not be sufficient. For example, if you are working on legal documents, there\n",
    "are words that may be frequently used on legal documents but not found in the public domain.\n",
    "So, there is a need to train the word vectors against these sets of documents.\n",
    "\n",
    "In this exercise, we will recreate the word vectors based on new set of news articles.\n",
    "- The source of the corpus is a text file in folder data.\n",
    "- The text file is articles.txt. One line contains one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-27T21:54:14.154063Z",
     "start_time": "2017-07-27T21:54:09.408786Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLP tools\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "# Data tools\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One: Train a custom word2ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Prepare the text corpus\n",
    "* Read the document into a Dataframe.\n",
    "* Split documents into sentences\n",
    "* Split each document into a list of words since GENSIM requires the documents to be represented as a list of sentences to train Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "articles_data = pd.read_csv('./data/articles.txt', sep='\\t',  header=None, names=[ \"text\"])                         \n",
    "# articles_data[0:3]   # display first three documents\n",
    "\n",
    "sentences = articles_data.text.str.split()   # Generate sentences for training word2vec\n",
    "#print (sentences[0])   # Check the content of the first documeht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Word2Vec model.\n",
    "Here are some important hyper parameters to consider\n",
    "* size - number of dimensions to represent words\n",
    "* window - context window size - distance between a target word and words around it\n",
    "* min_count - minimal number of words to consider in the corpus\n",
    "* workers - number of CPU core to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100  \n",
    "window = 5\n",
    "min_count = 1\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A\n",
    "\n",
    "* Instantial the Word2Vec model with the right object.\n",
    "* Build up the Word2Vec model. Take a stretch, it will take a while.\n",
    "* The tuning of the hyperparameter is out of scope for this lab. We will keep it for a more advance asession.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "custom_model = Word2Vec(sentences, size=size, window=window, min_count=min_count, workers=workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model as a file. This can be loaded in for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.save(\"custom_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=211201, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We like to see the first 100 words that are trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Barclays'\", 'defiance', 'of', 'US', 'fines', 'has', 'merit', 'Barclays', 'disgraced', 'itself', 'in', 'many', 'ways', 'during', 'the', 'pre-financial', 'crisis', 'boom', 'years.', 'So', 'it', 'is', 'tempting', 'to', 'think', 'bank,', 'when', 'asked', 'by', 'Department', 'Justice', 'pay', 'a', 'large', 'bill', 'for', 'polluting', 'financial', 'system', 'with', 'mortgage', 'junk', 'between', '2005', 'and', '2007,', 'should', 'cough', 'up,', 'apologise', 'learn', 'some', 'humility.', 'That', 'not', 'view', 'chief', 'executive,', 'Jes', 'Staley.', 'thinks', 'DoJ’s', 'claims', 'are', '“disconnected', 'from', 'facts”', 'that', '“an', 'obligation', 'our', 'shareholders,', 'customers,', 'clients', 'employees', 'defend', 'ourselves', 'against', 'unreasonable', 'allegations', 'demands.”', 'The', 'stance', 'possibly', 'foolhardy,', 'since', 'going', 'into', 'open', 'legal', 'battle', 'most', 'powerful', 'prosecutor', 'risky,', 'especially', 'if', 'you', 'end', 'up']\n"
     ]
    }
   ],
   "source": [
    "words = list(custom_model.wv.vocab)\n",
    "print(words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Let's see the embedded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01378442 -1.1174667   0.4744351  -1.2910578   0.52911615  1.5561563\n",
      " -0.14484563 -0.47406304  0.645721   -1.2883312   1.3062688   0.08798663\n",
      " -0.5034557   1.0442336  -0.7617575  -0.33152142 -1.0958656   0.81533015\n",
      "  0.15853634  0.5193816   2.5061345  -1.9462053   0.98096704 -0.80896395\n",
      " -0.55758625  0.86176926 -1.5865052  -0.70564646 -0.69579375  1.3621786\n",
      "  0.3874463  -1.4278525  -1.4715601  -1.7420949   0.5578193  -0.32139802\n",
      " -0.44999853 -0.77638054 -0.39046544 -1.4123049   0.9775229   1.4130435\n",
      "  0.1332317  -0.08742216  0.60607463 -0.24934907 -0.19561014  0.3671871\n",
      "  1.5404122   3.2982652   0.295346   -0.5780479   0.9804446  -0.31606913\n",
      " -0.32888243 -1.6041582   0.50948507  4.1281695  -1.2953583   0.07255799\n",
      "  0.84527147  0.04349367  0.23306443  0.007578    2.4469492  -0.00435588\n",
      "  0.36843035  0.27085546 -0.7627477   1.049201    0.7159596  -0.331684\n",
      "  0.12164953  0.5501899   1.3465499   1.4330037  -0.24350229  1.6792712\n",
      "  0.32269552  0.50986665  0.75697035 -0.28999075 -0.99770254  1.0134726\n",
      "  0.8546133   1.4592203  -1.8306798   0.582644    1.9562217   2.2731812\n",
      "  0.69185334 -0.34897098  0.60244036 -0.57897025 -2.325122    0.81909895\n",
      "  0.99600536 -1.338764    1.3798434   2.047914  ]\n"
     ]
    }
   ],
   "source": [
    "my_vector = custom_model.wv['customers']\n",
    "print(my_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part Two:  Add more words to custom model\n",
    "It is fair to assume that the word vector needs to be 'improved' upon subsequently when more documents are available. We can add more documents to a model as it evolves. Each document has to be a string. All the documents need to be in a list, followed by removal of stop words and then tokenized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Overseas REIT That Grew Year-on-Year DPU\", \n",
    "         \"Attention! Activist REIT Investors Needed in Singapore\",\n",
    "         \"Forget Physical Property. Buy These REIT Businesses to Gain Safer Exposure to Real Estate\",\n",
    "         \"Invest into REIT for stable income\",\n",
    "         \"REIT for retirement and property income\",\n",
    "         \"Property investment using REIT\",\n",
    "         \"Low Risk REIT for your retirement portfolio\",\n",
    "         \"Using REIT To get a steady Income\",\n",
    "         \"Multiple Income stream using REIT\",\n",
    "         \"Supplement Income through property securitied\",\n",
    "          \"Famous for been for fast person to make money in 3 years\"\n",
    "          \"Invest in propertly and get Passive Income using REIT\",\"Income without effort through REIT\",\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B\n",
    "Based on the example in the earlier part of the notebook, prepare the docs list into a structure that is ready to accept by GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_docs = []\n",
    "#your codes\n",
    "for doc in docs:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C\n",
    "Reload an existing Word2Vec model for addition training\n",
    "* Reload  the saved model\n",
    "* Summarized the loaded data. Note the vocabulary size\n",
    "* Get the vocabulary from in the pre-trained model (hint: attribute model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your  codes\n",
    "new_model = Word2Vec.load(\"custom_word2vec.model\")\n",
    "print(new_model)\n",
    "pretrained_words = list(new_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D\n",
    "We will proceed to train the model with the new set of document tokens\n",
    "* Build up the new vocabulary  (hint: Use the build_vocab() method )\n",
    "* Train the model with the new sentences (hint: Use the train() method\n",
    "* Summarized the loaded data. Note the new vocabulary size\n",
    "* Get the list of words in the new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.????????(????????, update=True)  ## prepare the model vocabulary\n",
    "new_model.?????????(????????, total_examples= len(new_docs), epochs=50)\n",
    "print(new_model)\n",
    "posttrained_words = list(new_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the new vocabulary\n",
    "* We find the difference in the vocabulary before and after the new training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['securitied', 'Passive', 'Year-on-Year', 'DPU', 'Attention!', 'Property.', 'yearsInvest', 'REIT', 'propertly', 'Exposure']\n"
     ]
    }
   ],
   "source": [
    "# words = list(custom_model.wv.vocab)\n",
    "\n",
    "print(list(set(posttrained_words)-set(pretrained_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.96350794e-02 -2.02486129e-03 -4.10056971e-02  1.35938507e-02\n",
      " -2.50798557e-03 -4.30625584e-03 -1.67355291e-04 -2.45491881e-02\n",
      "  1.88353062e-02 -2.46145786e-03  9.69257235e-05  1.21400459e-02\n",
      " -2.23264080e-02 -1.00820260e-02 -1.99193843e-02 -9.01276898e-03\n",
      " -4.29931376e-03  1.43385250e-02 -1.17647955e-02  3.10437270e-02\n",
      " -1.93367358e-02  1.66339092e-02  1.41026732e-02 -1.56057533e-02\n",
      " -6.55088667e-03 -3.67482565e-02 -1.48438793e-02 -2.23036278e-02\n",
      " -5.23079466e-03 -4.36255801e-03  3.28454785e-02  4.21109609e-03\n",
      " -1.47005273e-02  3.54526043e-02 -3.81266070e-03 -1.38563309e-02\n",
      " -4.18880861e-03 -1.98773704e-02  3.29029933e-02 -4.01221123e-03\n",
      "  2.55043358e-02 -3.20761390e-02  2.69843079e-02 -8.36666208e-03\n",
      " -2.28520036e-02 -1.22945150e-02 -6.97563728e-03  7.71008292e-03\n",
      " -2.87611084e-03 -1.08578959e-02 -5.44355474e-02  9.74336435e-05\n",
      "  3.62618989e-03 -1.40412757e-03  3.54965702e-02  1.26089267e-02\n",
      " -3.65837254e-02  2.55953986e-02 -4.76709791e-02 -1.44040212e-02\n",
      " -1.88120827e-02  2.75070425e-02  2.95322612e-02  4.54915725e-02\n",
      "  5.71501255e-03  1.87419355e-02 -2.44158041e-02 -1.58297382e-02\n",
      "  1.79490428e-02 -1.86883360e-02  1.21762371e-02  3.08149843e-03\n",
      "  4.55070054e-03  8.36818479e-03  1.16217334e-03  1.70946028e-02\n",
      " -1.48269702e-02  1.60552859e-02 -2.68544592e-02  1.77632067e-02\n",
      "  1.77139938e-02 -3.07452455e-02 -2.07582898e-02  1.00977405e-03\n",
      "  5.84700785e-04  3.44573939e-03 -1.50868390e-02  2.87211388e-02\n",
      "  3.42892483e-02 -1.98423071e-03 -1.83830820e-02 -2.92090774e-02\n",
      " -1.12940250e-02 -1.52361877e-02 -6.70749843e-02  3.67189236e-02\n",
      " -1.73899829e-02 -4.73308414e-02  7.51559623e-03  1.54113024e-02]\n"
     ]
    }
   ],
   "source": [
    "#new_model.word_vec('REIT')\n",
    "v = new_model.wv.word_vec('REIT')\n",
    "print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final model and the key vectors.\n",
    "When the modelling is completed, you may wish to save the vectors into the KeyedVectored struture.\n",
    "\n",
    "The reason for separating the trained vectors into KeyedVectors is that if you don’t need the full model state any more (don’t need to continue training), the state can discarded, resulting in a much smaller and faster object that can be mmapped for lightning fast loading and sharing the vectors in RAM between processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.wv.save('new_custom_word2vec.kv')   # This saves the word-vectors lookup\n",
    "new_model.save(\"new_custom_word2vec.model\")   # this saves the model for future training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved KeyedVector can be loaded in subsequently for application use. A sample code to load in is shown below. the vector for a word is also retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1a7d0416d8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"new_custom_word2vec.kv\", mmap='r')\n",
    "wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46485904 -1.0854895  -0.422736    0.63330615 -0.01367772 -0.19758989\n",
      "  0.17786923 -1.1056702   0.44522965 -0.34253973  0.2706924   0.30083555\n",
      "  0.08683466 -0.13762787 -0.6585511  -0.02505613 -0.6359403  -0.23481385\n",
      " -0.45187244  0.8636467  -0.35240334 -0.45184505  0.24784958  0.28864238\n",
      "  0.09729365 -0.5539774  -0.4438754   0.02398845  0.15363957 -0.431226\n",
      "  0.18322936  0.06095865 -0.8781888   0.02536917 -0.1797969  -1.1510798\n",
      " -0.37910458 -0.32920897  0.9193784  -0.03638902 -0.04682702 -0.8927201\n",
      " -0.12078495 -0.50178176  0.27532578 -0.76397955 -0.14572047  0.1539461\n",
      "  0.38892302  0.38636476 -0.42555583 -0.39981484  0.37844056 -0.44468844\n",
      " -0.05397031  0.1801081  -0.45894274  0.49194783 -0.61313987 -0.18911152\n",
      "  0.35393918 -0.07444182  0.3684309   0.30126336 -0.04820593 -0.00197545\n",
      " -0.7725796  -0.05828528 -0.5999901  -0.18389821  0.3623138  -0.57996565\n",
      "  0.5374159  -0.01805456 -0.57517934  0.44733483 -0.08229531 -0.10354503\n",
      " -0.44336882 -0.2573726  -0.52780646 -0.66958344 -0.5550492   0.68304145\n",
      "  0.26493967  0.3193732  -0.5549436  -0.08370224 -0.18252932  0.30970347\n",
      " -0.36765254 -0.7175691  -0.6867714  -0.47475076 -1.5267984   0.9645301\n",
      "  0.38367578 -0.298279    0.23422979  0.3848625 ]\n"
     ]
    }
   ],
   "source": [
    "vector = wv['computer']\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Excerise\n",
    "* Use the file cnus.txt as a new input to the model\n",
    "* Further train the Word2Vec model with the new file\n",
    "* Check if the vocaulary size has increased\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise\n",
    "It is worth examining if any word (e.g w1) in the initial training is found in the future documents. Check if the vector for w1 remains the same or changes after the subsequent training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
