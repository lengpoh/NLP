{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()  #<-- Run this if it's your first time using nltk to download all of the datasets and models\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import re # Regular expression library\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Mr. Smith!', 'I’m going to buy some vegetables (tomatoes and cucumbers) from the store.', 'Should I pick up some black-eyed peas as well?']\n"
     ]
    }
   ],
   "source": [
    "### sentence tokensization using sentences\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"\n",
    "print(sent_tokenize(my_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'Smith', '!', 'I', '’', 'm', 'going', 'to', 'buy', 'some', 'vegetables', '(', 'tomatoes', 'and', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
     ]
    }
   ],
   "source": [
    "#### Tokenization using words\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(word_tokenize(my_text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi', 'Mr.', 'Smith', '!'], ['I', '’', 'm', 'going', 'to', 'buy', 'some', 'vegetables', '(', 'tomatoes', 'and', 'cucumbers', ')', 'from', 'the', 'store', '.'], ['Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']]\n"
     ]
    }
   ],
   "source": [
    "### Tokenize from text to sentences to words\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"\n",
    "sentences = sent_tokenize(my_text)\n",
    "\n",
    "#print (sentences)\n",
    "my_text_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "   my_text_tokens.append(word_tokenize(sentence))  \n",
    "print (my_text_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'authorities'), ('authorities', 'fined'), ('fined', 'Google'), ('Google', 'a'), ('a', 'record'), ('record', '5.1'), ('5.1', 'billion'), ('billion', 'on'), ('on', 'Wednesday'), ('Wednesday', 'for'), ('for', 'abusing'), ('abusing', 'its'), ('its', 'power'), ('power', 'in'), ('in', 'the'), ('the', 'mobile'), ('mobile', 'phone'), ('phone', 'market')]\n"
     ]
    }
   ],
   "source": [
    "#### Tokenization in bigrams\n",
    "\n",
    "my_text = \"European authorities fined Google a record 5.1 billion on Wednesday for abusing its power in the mobile phone market\"\n",
    "\n",
    "\n",
    "bigram_mytext = list(ngrams(word_tokenize(my_text),2))\n",
    "print (bigram_mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'authorities'), ('authorities', 'fined'), ('fined', 'Google'), ('Google', 'a'), ('a', 'record'), ('record', '5.1'), ('5.1', 'billion'), ('billion', 'on'), ('on', 'Wednesday'), ('Wednesday', 'for'), ('for', 'abusing'), ('abusing', 'its'), ('its', 'power'), ('power', 'in'), ('in', 'the'), ('the', 'mobile'), ('mobile', 'phone'), ('phone', 'market')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenization in bigrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\u000b",
    "print(twograms)\n",
    "print (twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'authorities', 'fined', 'Google', 'a', 'record', '5.1', 'billion', 'on', 'Wednesday', 'for', 'abusing', 'its', 'power', 'in', 'the', 'mobile', 'phone', 'market']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expression to tokenize using white space\n",
    "\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(whitespace_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'Google', 'Wednesday']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_text : Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\n",
      "clean_text : Hi Mr  Smith  I’m going to buy some vegetables  tomatoes and cucumbers  from the store  Should I pick up some black eyed peas as well \n",
      "clean_text ; Hi Mr  Smith  I’m going to buy some vegetables  tomatoes and cucumbers  from the store  Should I pick up some black eyed peas as well \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace punctuations with a white space\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"\n",
    "clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "clean_text2 = re.sub(\"[.,\\/#!$%\\^&\\*;:?{}=\\-_`~()]\", \" \",  my_text)\n",
    "#replace(/[.,\\/#!$%\\^&\\*;:{}=\\-_`~()]/g,\"\")\n",
    "\n",
    "print(\"my_text :\", my_text)\n",
    "print(\"clean_text :\",clean_text)\n",
    "print(\"clean_text ;\",clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr  smith  i’m going to buy some vegetables  tomatoes and cucumbers  from the store  should i pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to lower case\n",
    "clean_text = clean_text.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr  smith  i’m going to buy some vegetables  tomatoes and cucumbers  from the store  should i pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", \"aren't\", 'the', 'she', 'just', \"won't\", 'hadn', \"hasn't\", \"should've\", 'that', 'did', 'it', 'below', 'for', 'not', 'nor', 'isn', 'this', 'which', 'wouldn', \"that'll\", 'haven', 'same', 'more', 'needn', 'before', 'has', 'didn', 'of', 'yours', 'but', 'your', 'had', 'what', 'you', 'down', 'during', 'his', 'on', \"mustn't\", \"hadn't\", 'only', \"haven't\", 'as', 'few', 'him', 'most', \"shouldn't\", 'me', 'a', \"mightn't\", 'over', 'in', 'an', 'hers', 'after', 'being', 'under', 'own', 'each', \"wouldn't\", 'been', 'again', 'so', 'they', 'wasn', 'll', 's', 'then', \"wasn't\", 'into', 'is', 'doesn', 'o', 'its', 'aren', 'above', 'can', 'these', 'OMG', 'too', 'doing', 'will', 'through', 'until', 'mightn', 'we', 'no', \"didn't\", 'to', \"shan't\", 'at', 'their', 'when', 'whom', 'very', 'ours', 'themselves', 'up', \"doesn't\", 'having', 'don', 'yourselves', 'if', 'here', 'd', 'won', 'couldn', 'by', 'or', 'there', 'who', 'from', 'be', \"you've\", \"you'd\", 'any', 'both', 'himself', 'does', \"weren't\", 'other', 'how', 'theirs', 'once', 'and', 've', 'are', 'them', 'am', 'hasn', 'between', \"don't\", 'such', 'than', 'ourselves', 'were', 'y', 'ain', 'do', 'further', \"isn't\", 'about', 'all', 're', 'i', \"you'll\", 'my', \"you're\", \"she's\", 'out', 'where', 'our', 'why', 't', \"it's\", 'while', 'off', 'some', 'her', 'weren', 'mustn', 'shan', 'against', 'shouldn', 'with', 'ma', 'herself', 'myself', 'should', \"couldn't\", 'now', 'he', 'was', 'm', 'those', 'yourself', 'itself', 'have'}\n"
     ]
    }
   ],
   "source": [
    "# Get stop words from NLTK\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "my_stopwords.add('OMG')\n",
    "my_stopwords.remove('because')\n",
    "print (my_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word tokens:  ['the', 'product', 'is', 'not', 'good']\n",
      "Filtered word tokens:  ['product', 'good']\n"
     ]
    }
   ],
   "source": [
    "## danger of mindlessly removing stopwords\n",
    "\n",
    "text = 'the product is not good'\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#stop_words.remove('not')\n",
    "word_tokens = word_tokenize(text) \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    " \n",
    "print(\"Original word tokens: \" , word_tokens) \n",
    "print(\"Filtered word tokens: \" , filtered_sentence) \n",
    "\n",
    "# To resolve this above problem, you may have to remove the stop word \"NOT\" from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: driv\n",
      "drives: driv\n",
      "driver: driv\n",
      "drivers: driv\n",
      "driven: driv\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# Try some stems\n",
    "print('drive: {}'.format(stemmer.stem('drive')))\n",
    "print('drives: {}'.format(stemmer.stem('drives')))\n",
    "print('driver: {}'.format(stemmer.stem('driver')))\n",
    "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
    "print('driven: {}'.format(stemmer.stem('driven')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'NNP'), ('Smith', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part of speech\n",
    "#nltk.download('tagsets')\n",
    "from nltk.tag import pos_tag\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# If you need to know what the tag means\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complex words\n",
    "\n",
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word tokens:  ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Filtered word tokens:  ['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(example_sent) \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    " \n",
    "print(\"Original word tokens: \" , word_tokens) \n",
    "print(\"Filtered word tokens: \" , filtered_sentence) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", \"aren't\", 'the', 'she', 'just', \"won't\", 'hadn', \"hasn't\", \"should've\", 'that', 'did', 'it', 'below', 'for', 'not', 'nor', 'isn', 'this', 'which', 'wouldn', \"that'll\", 'haven', 'same', 'more', 'needn', 'before', 'has', 'didn', 'of', 'yours', 'but', 'your', 'had', 'what', 'you', 'down', 'during', 'his', 'on', \"mustn't\", \"hadn't\", 'only', \"haven't\", 'as', 'few', 'him', 'most', \"shouldn't\", 'me', 'a', \"mightn't\", 'over', 'in', 'an', 'hers', 'after', 'being', 'under', 'own', 'each', \"wouldn't\", 'been', 'again', 'so', 'they', 'wasn', 'because', 'll', 's', 'then', \"wasn't\", 'into', 'is', 'doesn', 'o', 'its', 'aren', 'above', 'can', 'these', 'too', 'doing', 'will', 'through', 'until', 'mightn', 'we', 'no', \"didn't\", 'to', \"shan't\", 'at', 'their', 'when', 'whom', 'very', 'ours', 'themselves', 'up', \"doesn't\", 'having', 'don', 'yourselves', 'if', 'here', 'd', 'won', 'couldn', 'by', 'or', 'there', 'who', 'from', 'be', \"you've\", \"you'd\", 'any', 'both', 'himself', 'does', \"weren't\", 'other', 'how', 'theirs', 'once', 'and', 've', 'are', 'them', 'am', 'hasn', 'between', \"don't\", 'such', 'than', 'ourselves', 'were', 'y', 'ain', 'do', 'further', \"isn't\", 'about', 'all', 're', 'i', \"you'll\", 'my', \"you're\", \"she's\", 'out', 'where', 'our', 'why', 't', \"it's\", 'while', 'off', 'some', 'her', 'weren', 'mustn', 'shan', 'against', 'shouldn', 'with', 'ma', 'herself', 'myself', 'should', \"couldn't\", 'now', 'he', 'was', 'm', 'those', 'yourself', 'itself', 'have'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "print (my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", \"aren't\", 'the', 'she', 'just', \"won't\", 'hadn', \"hasn't\", \"should've\", 'that', 'did', 'it', 'below', 'for', 'not', 'nor', 'isn', 'this', 'which', 'wouldn', \"that'll\", 'haven', 'same', 'more', 'needn', 'before', 'has', 'didn', 'of', 'yours', 'but', 'your', 'had', 'what', 'you', 'down', 'during', 'his', 'on', \"mustn't\", \"hadn't\", 'only', \"haven't\", 'as', 'few', 'him', 'most', \"shouldn't\", 'me', 'a', \"mightn't\", 'over', 'in', 'an', 'hers', 'after', 'being', 'under', 'own', 'each', \"wouldn't\", 'been', 'again', 'so', 'they', 'wasn', 'because', 'll', 's', 'then', \"wasn't\", 'into', 'is', 'doesn', 'o', 'its', 'aren', 'above', 'can', 'these', 'OMG', 'too', 'doing', 'will', 'through', 'until', 'mightn', 'we', 'no', \"didn't\", 'to', \"shan't\", 'at', 'their', 'when', 'whom', 'very', 'ours', 'themselves', 'up', \"doesn't\", 'having', 'don', 'yourselves', 'if', 'here', 'd', 'won', 'couldn', 'by', 'or', 'there', 'who', 'from', 'be', \"you've\", \"you'd\", 'any', 'both', 'himself', 'does', \"weren't\", 'other', 'how', 'theirs', 'once', 'and', 've', 'are', 'them', 'am', 'hasn', 'between', \"don't\", 'such', 'than', 'ourselves', 'were', 'y', 'ain', 'do', 'further', \"isn't\", 'about', 'all', 're', 'i', \"you'll\", 'my', \"you're\", \"she's\", 'out', 'where', 'our', 'why', 't', \"it's\", 'while', 'off', 'some', 'her', 'weren', 'mustn', 'shan', 'against', 'shouldn', 'with', 'ma', 'herself', 'myself', 'should', \"couldn't\", 'now', 'he', 'was', 'm', 'those', 'yourself', 'itself', 'have'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "my_stopwords = set(stopwords.words('english'))\n",
    "my_stopwords.add('OMG')\n",
    "print (my_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "words = [\"car\", \"cars\", \"care\", \"caring\", \"careful\", \"boats\", \"boating\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatise car  --> car\n",
      "Lemmatise cars  --> car\n",
      "Lemmatise care  --> care\n",
      "Lemmatise caring  --> caring\n",
      "Lemmatise careful  --> careful\n",
      "Lemmatise boats  --> boat\n",
      "Lemmatise boating  --> boating\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "   print(\"Lemmatise %s  --> %s\" % (word, lemmatiser.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatise car  --> car\n",
      "Lemmatise cars  --> car\n",
      "Lemmatise care  --> care\n",
      "Lemmatise caring  --> caring\n",
      "Lemmatise careful  --> careful\n",
      "Lemmatise boats  --> boat\n",
      "Lemmatise boating  --> boating\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for word in words:\n",
    "   print(\"Lemmatise %s  --> %s\" % (word, lemmatiser.lemmatize(word, pos=\"n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatise car --> car\n",
      "Lemmatise cars --> cars\n",
      "Lemmatise care --> care\n",
      "Lemmatise caring --> care\n",
      "Lemmatise careful --> careful\n",
      "Lemmatise boats --> boat\n",
      "Lemmatise boating --> boat\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "   print(\"Lemmatise %s --> %s\" % (word, lemmatiser.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "best : best\n",
      "better : good\n",
      "best : best\n",
      "transport : transport\n",
      "transport : transportation\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "\n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "print(\"best :\", lemmatizer.lemmatize(\"best\", pos =\"a\"))\n",
    "\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) \n",
    "print(\"best :\", lemmatizer.lemmatize(\"best\", pos =\"a\"))\n",
    "\n",
    "print(\"transport :\", lemmatizer.lemmatize(\"transported\", pos =\"v\")) \n",
    "print(\"transport :\", lemmatizer.lemmatize(\"transportation\", pos =\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('European', 'JJ')\n",
      "('authorities', 'NNS')\n",
      "('fined', 'VBD')\n",
      "('Google', 'NNP')\n",
      "('a', 'DT')\n",
      "('record', 'NN')\n",
      "('5.1', 'CD')\n",
      "('billion', 'CD')\n",
      "('on', 'IN')\n",
      "('Wednesday', 'NNP')\n",
      "('for', 'IN')\n",
      "('abusing', 'VBG')\n",
      "('its', 'PRP$')\n",
      "('power', 'NN')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('mobile', 'JJ')\n",
      "('phone', 'NN')\n",
      "('market', 'NN')\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "    \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"European authorities fined Google a record 5.1 billion on Wednesday for abusing its power in the mobile phone market\"\n",
    "\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(text)\n",
    "sent = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "for s in sent:\n",
    "    print (s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'authorities', 'fined', 'Google', 'a', 'record', '5.1', 'billion', 'on', 'Wednesday', 'for', 'abusing', 'its', 'power', 'in', 'the', 'mobile', 'phone', 'market']\n"
     ]
    }
   ],
   "source": [
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens ->  ['I', 'am', 'interested', 'in', 'data', 'science', 'and', 'artificial', 'intelligence']\n",
      "Compound words tokens ->  ['I', 'am', 'interested', 'in', 'data_science', 'and', 'artificial_intelligence']\n"
     ]
    }
   ],
   "source": [
    "# Example on Extracting Compound Words\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "text = \"I am interested in data science and artificial intelligence\"\n",
    "mwe_tokenizer = MWETokenizer([(\"artificial\",\"intelligence\"), (\"data\",\"science\")], separator='_')\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
    "\n",
    "print (\"Word tokens -> \", word_tokens)\n",
    "print(\"Compound words tokens -> \", mwe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "None\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "None\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for tag in ['NNP', 'VBZ', 'DT']:\n",
    "    print(nltk.help.upenn_tagset(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
